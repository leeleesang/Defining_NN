{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a8c854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "This is experiment with random label, random_ratio: 0.0, Goal accuracy: 99.99\n",
      "Class Overlapped : 6742 / 6742\n"
     ]
    }
   ],
   "source": [
    "# Classification, Intrinsic dimension, Degeneracy\n",
    "%run SetUp.ipynb\n",
    "\n",
    "## Configuration \n",
    "Title = \"Rebuttal\" # general two layer\n",
    "device = \"cuda:1\"\n",
    "dataset_type = \"Training set\" # \"Training + Test set\"\n",
    "data_balace = 1\n",
    "selected_class = 1 ################ selected clas ##### \n",
    "\n",
    "pt_option=False\n",
    "dataset = \"MNIST\"\n",
    "random_ratio = 0/100   ## 001 / 010 / 025 / 050 / 075 / 100\n",
    "width = 10\n",
    "width_increment = 30\n",
    "\n",
    "\n",
    "###############################\n",
    "num_classes = 2 # binary classification\n",
    "Goal_Accuracy = 99.99\n",
    "first_positive_init = False ## if False, this approximate the class : {selected_class}\n",
    "############################### if Ture, approximate the complement : {selected_class}^c\n",
    "\n",
    "\n",
    "if dataset == \"MNIST\":\n",
    "    d = 28*28\n",
    "    n = 60000\n",
    "    data_balance = 1 ## the lambda value\n",
    "    Epochs = 100*1000\n",
    "    start_pruning = 9999 # pretraining\n",
    "\n",
    "elif dataset == \"FashionMNIST\":\n",
    "    d = 28*28\n",
    "    n = 60000\n",
    "    data_balance = 1 ## the lambda value\n",
    "    Epochs = 100*1000\n",
    "    start_pruning = 9999 # pretraining\n",
    "\n",
    "elif dataset == \"CIFAR10\":\n",
    "    d = 3*32*32\n",
    "    n = 50000\n",
    "    data_balance = 1 ## the lambda value\n",
    "    Epochs = 100*1000\n",
    "    start_pruning = 9999 # pretraining\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Wrong Dataset !\")\n",
    "# n = 1000 # for debug, if need\n",
    "\n",
    "\n",
    "\n",
    "# create directory\n",
    "output_path = f\"./Results/{Title}\"\n",
    "create_directory(output_path)\n",
    "# construct the dataset\n",
    "trn_X, trn_Y, test_X, test_Y = load_dataset(dataset, n)\n",
    "\n",
    "\n",
    "if dataset_type == \"Training + Test set\" : \n",
    "    ########### trn + test set #############\n",
    "    trn_X = torch.cat((trn_X, test_X)) ##### combine test set and trn set !\n",
    "    trn_Y = torch.cat((trn_Y, test_Y)) ##### \n",
    "    ########################################\n",
    "elif dataset_type == \"Training set\":\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(f\"Wrong type for the dataset_type : {dataset_type}\")\n",
    "\n",
    "\n",
    "trn_Y = (trn_Y == selected_class).float()# change to binary class ## label 1 for the selected class.\n",
    "test_Y = (test_Y == selected_class).float()\n",
    "n = len(trn_X)\n",
    "\n",
    "\n",
    "###################################\n",
    "####### random label ##############\n",
    "print(f\"\\n\\n\\nThis is experiment with random label, random_ratio: {random_ratio}, Goal accuracy: {Goal_Accuracy}\")\n",
    "trn_Y_clone = trn_Y.clone()\n",
    "_k = 0\n",
    "while True:\n",
    "    trn_Y_clone = trn_Y.clone()\n",
    "    random_Y = ( torch.rand(len(trn_Y)).to(device) < (trn_Y.sum() / len(trn_Y)) ).float().to(device) ######################### random dataset !!!!\n",
    "    random_index = (torch.rand(len(trn_Y)) < random_ratio)\n",
    "    trn_Y_clone[random_index] = random_Y[random_index].clone()\n",
    "    _k+=1\n",
    "    if trn_Y_clone.sum() == len(trn_Y[trn_Y>0]) :\n",
    "        class_overlapped = (trn_Y[trn_Y==1] == trn_Y_clone[trn_Y==1]).sum().item()\n",
    "        print(\"Class Overlapped :\", class_overlapped, \"/\", len(trn_Y[trn_Y==1]))\n",
    "        trn_Y = trn_Y_clone\n",
    "#         print(trn_Y_clone.sum())\n",
    "#         print(_k)\n",
    "        break\n",
    "##########################################\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c207e845",
   "metadata": {},
   "source": [
    "Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca0f0f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Network setting # d -> d1 -> 1\n",
    "class polytope(nn.Module):\n",
    "    def __init__(self, width, output_class=1, positive_init=False, small_norm_init=False):\n",
    "        super(polytope, self).__init__()\n",
    "        self.fc0 = nn.Linear(d, width)\n",
    "        self.fc1 = nn.Linear(width, output_class, bias=False)\n",
    "        self.width = width # width\n",
    "        self.epoch = 0\n",
    "        self.data_balance = data_balance\n",
    "        self.bias = (1 - 2*positive_init)*5 # +-5\n",
    "        self.positive_init = positive_init\n",
    "        self.output_class = output_class\n",
    "        self.small_norm_init = small_norm_init\n",
    "        self.device = device\n",
    "        \n",
    "        # initialization\n",
    "        if small_norm_init:\n",
    "            self.change_layer_weights(layer=0, W=1.5/self.width*torch.randn_like(self.W(0)), b=0.6/self.width*(self.b(0))) # multiply 0.01 \n",
    "            \n",
    "        if positive_init:\n",
    "            # if positivie init,  all v_k are positive.\n",
    "            self.fc1.weight = nn.Parameter(torch.sqrt((self.W(0).norm(dim=1)**2 + self.b(0)**2).view(1,-1)+1))\n",
    "        else:\n",
    "            # else, to all v_k are negative.\n",
    "            self.fc1.weight = nn.Parameter(-torch.sqrt((self.W(0).norm(dim=1)**2 + self.b(0)**2).view(1,-1)+1))\n",
    "        \n",
    "        # declare the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.g1 = self.fc0(x)\n",
    "        self.h1 = F.relu(self.g1)\n",
    "        self.g2 = self.fc1(self.h1) + self.bias\n",
    "        return self.g2.squeeze()\n",
    "    \n",
    "    def W(self, layer):\n",
    "        if layer ==0:\n",
    "            output = self.fc0.weight\n",
    "        elif layer ==1:\n",
    "            output = self.fc1.weight\n",
    "        return output.clone()\n",
    "    def b(self, layer):\n",
    "        if layer ==0:\n",
    "            output = self.fc0.bias\n",
    "        elif layer ==1:\n",
    "            output = self.fc1.bias\n",
    "        return output if output == None else output.clone()\n",
    "    \n",
    "    def activation_pattern(self, x):\n",
    "        self.forward(x)\n",
    "        return (self.h1>0).float()\n",
    "    \n",
    "    def change_layer_weights(self, layer, W, b):\n",
    "        if self.b(layer) == None : # no bias term\n",
    "            if W.shape == self.W(layer).shape and self.b(layer) == b:\n",
    "                if layer ==0:\n",
    "                    self.fc0.weight = nn.Parameter(W.to(self.device))\n",
    "                elif layer ==1:\n",
    "                    self.fc1.weight = nn.Parameter(W.to(self.device))\n",
    "            else:\n",
    "                raise ValueError(\"wrong shape of input tensors\")\n",
    "        else:\n",
    "            if W.shape == self.W(layer).shape and b.shape == self.b(layer).shape:\n",
    "                if layer ==0:\n",
    "                    self.fc0.weight = nn.Parameter(W.to(self.device))\n",
    "                    self.fc0.bias = nn.Parameter(b.to(self.device))\n",
    "                elif layer ==1:\n",
    "                    self.fc1.weight = nn.Parameter(W.to(self.device))\n",
    "    #                 self.fc1.bias = nn.Parameter(b)\n",
    "            else:\n",
    "                raise ValueError(\"wrong shape of input tensors\")\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    def train(self, repetition, trn_X_index):\n",
    "        self.fail = False\n",
    "        self.trn_loss_tr = np.empty(0)\n",
    "        self.test_loss_tr = np.empty(0)\n",
    "        self.trn_acc_tr = np.empty(0)\n",
    "        self.test_acc_tr = np.empty(0)\n",
    "\n",
    "        class0_outPoly_min = 1\n",
    "        test_acc = 0\n",
    "        test_loss = torch.zeros(1)\n",
    "        print(f\"=================================   Start Training. Repetition: {repetition+1}   ==================================\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        log_period = 500\n",
    "        for epoch in tqdm(range(Epochs)) :\n",
    "            self.epoch = epoch\n",
    "            \n",
    "            # since class may be imbalance\n",
    "            loss =  criterion(net(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(positive_init)).view(-1)]).squeeze(),\n",
    "                              trn_Y[trn_X_index][(trn_Y[trn_X_index]==float(positive_init)).view(-1)]) / len((trn_Y[trn_X_index]==float(positive_init)).view(-1))\n",
    "            loss += criterion(net(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(not positive_init)).view(-1)]).squeeze(), \n",
    "                              trn_Y[trn_X_index][(trn_Y[trn_X_index]==float(not positive_init)).view(-1)]) / len((trn_Y[trn_X_index]==float(not positive_init)).view(-1)) * self.data_balance\n",
    "            loss *= len((trn_Y[trn_X_index]).view(-1))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "#             self.pruning(trn_X[trn_X_index])\n",
    "\n",
    "\n",
    "            if epoch ==0 or epoch % log_period == (log_period-1) :\n",
    "                self.trn_loss_tr = np.append(self.trn_loss_tr, loss.item())\n",
    "                self.test_loss_tr = np.append(self.test_loss_tr, test_loss.item())\n",
    "\n",
    "                trn_acc = ((net(trn_X[trn_X_index])>0).float() == trn_Y[trn_X_index]).sum().item() / len(trn_X[trn_X_index]) *100\n",
    "                self.trn_acc_tr = np.append(self.trn_acc_tr, trn_acc)\n",
    "                self.test_acc_tr = np.append(self.test_acc_tr, test_acc)\n",
    "                class0_inPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]!=float(self.positive_init)).view(-1)]) == self.bias).sum().item()\n",
    "                class1_inPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(self.positive_init)).view(-1)]) == self.bias).sum().item()\n",
    "                class0_outPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]!=float(self.positive_init)).view(-1)]) != self.bias).sum().item()\n",
    "                class1_outPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(self.positive_init)).view(-1)]) != self.bias).sum().item()\n",
    "                writer.add_scalars(\"ReLU/Loss\", {\n",
    "                    \"Trn_Loss\": loss.item(),\n",
    "                    \"Test_Loss\": test_loss.item(), }\n",
    "                                   , epoch+1)\n",
    "                writer.add_scalars(\"ReLU/Accuracy\", {\n",
    "                    \"Trn_acc\": trn_acc,\n",
    "                    \"Test_acc\": test_acc, }\n",
    "                                   , epoch+1)\n",
    "                writer.add_scalars(\"ReLU/InOutClass\", {\n",
    "                    \"class0_inPoly\": class0_inPoly,\n",
    "                    \"class0_outPoly\": class0_outPoly,\n",
    "                    \"class0_inPoly\": class1_inPoly,\n",
    "                    \"class0_outPoly\": class1_outPoly,}\n",
    "                                   , epoch+1)\n",
    "                if pt_option :\n",
    "                    print(f\"Epoch: {epoch+1 :>5} | width: {self.width} | loss: {loss.item() :.4f} | ACC: {trn_acc:.3f}\", end=\"\")\n",
    "                    print(f\" | Wmin: {self.fc1.weight.abs().min():.1f} | class0_inPoly : {class0_inPoly}\", end=\"\")\n",
    "                    print(f\" | class0_outPoly : {class0_outPoly} | class1_inPoly : {class1_inPoly} | class1_outPoly : {class1_outPoly}\")\n",
    "                \n",
    "                if class0_outPoly == 0: # a good network. save.\n",
    "                    class0_outPoly_min = 0\n",
    "                    torch.save(self.state_dict(), folder_name+f\"/Rep{repetition+1}_saved_net_width_{width}_min.pt\") # save a good polytope\n",
    "                torch.save(self.state_dict(), folder_name+f\"/Rep{repetition+1}_saved_net_width_{width}.pt\") # save a good polytope\n",
    "                \n",
    "                if self.fc1.weight.sign().sum().abs() !=  self.width: # there is a flipped sign.\n",
    "                    print(\"!!! Sign flipped !!\") \n",
    "                    self.fail = True # skip, train a new network for this round\n",
    "                    break\n",
    "                \n",
    "                if self.width == 0 : # there is no active neuron.\n",
    "                    print(\"!!! All neruons have been removed !!\") \n",
    "                    self.fail = True # skip, train a new network for this round\n",
    "                    break\n",
    "                \n",
    "                if (class1_inPoly > (100 - Goal_Accuracy)/100 * len(trn_Y)) and (epoch > 5000) :\n",
    "                    print(f\"Fail, more than {(100 - Goal_Accuracy) :.2f}% wrong class in the polytope with width {self.width}\") \n",
    "                    self.fail = True\n",
    "                    break\n",
    "                    \n",
    "\n",
    "                \n",
    "                if (n - class0_outPoly - class1_inPoly) / n * 100 >= Goal_Accuracy :\n",
    "                    print(f\"Found a {Goal_Accuracy}% convex polytope cover with width {self.width}, escape the training loop\") \n",
    "                    self.fail = False\n",
    "                    break\n",
    "\n",
    "                # Pruning and merging\n",
    "                self.pruning(trn_X[trn_X_index]) if epoch > start_pruning else None ## pruning after pre-training\n",
    "        \n",
    "        if class0_outPoly_min ==0:\n",
    "            self.load_state_dict(torch.load(folder_name+f\"/Rep{repetition+1}_saved_net_width_{width}_min.pt\")) # save a good polytope\n",
    "        \n",
    "        # if it fails to achieve convergence : \n",
    "        if (class0_outPoly > (100 - Goal_Accuracy)/100 * len(trn_Y) ):\n",
    "            print(f\"!!! It is failed to find a perfect polytope with width {self.width}!!!!\") \n",
    "            self.fail = True # skip, train a new network for this round\n",
    "#         else: # load the saved network with a good polytope\n",
    "#             self.load_state_dict(torch.load(folder_name+f\"/Rep{repetition+1}_saved_net_width_{width}.pt\")) # save a good polytope\n",
    "# #             self.eval()\n",
    "                                 \n",
    "        self.save_loss_graph()\n",
    "        self.save_result_txt()\n",
    "        time.sleep(1)\n",
    "        print(f\"===================================   Training finished, Repetition: {repetition+1}  ================================== \\n\\n\")\n",
    "        \n",
    "    \n",
    "    def save_loss_graph(self):\n",
    "        # plot and save the graphs\n",
    "        # PLOT THE LOSS GRAPH\n",
    "        plt.plot(self.trn_loss_tr, label=\"Train\")\n",
    "        plt.plot(self.test_loss_tr, label=\"Test\")\n",
    "        plt.xlabel(\"Iterations (k)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Rep{repetition+1}_Loss\")\n",
    "        plt.savefig(folder_name+\"/loss_fig_fullview.png\")\n",
    "#         plt.show() if pt_option else None\n",
    "        plt.close()\n",
    "\n",
    "        # PLOT THE ACCURACY GRAPH\n",
    "        plt.plot(self.trn_acc_tr, label=\"Train\")\n",
    "        plt.plot(self.test_acc_tr, label=\"Test\")\n",
    "        plt.xlabel(\"Iterations (k)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Rep{repetition+1}_Accuracy\")\n",
    "        plt.savefig(folder_name+\"/Accuracy.png\")\n",
    "#         plt.show() if pt_option else None\n",
    "        plt.close()\n",
    "    \n",
    "    # Save result txt file.\n",
    "    def save_result_txt(self):\n",
    "        class0_inPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]!=float(self.positive_init)).view(-1)]) == self.bias).sum().item()\n",
    "        class1_inPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(self.positive_init)).view(-1)]) == self.bias).sum().item()\n",
    "        class0_outPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]!=float(self.positive_init)).view(-1)]) != self.bias).sum().item()\n",
    "        class1_outPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(self.positive_init)).view(-1)]) != self.bias).sum().item()\n",
    "        \n",
    "        f = open(f\"./{folder_name}/Rep{repetition+1}_result.txt\", 'w')\n",
    "        f.write(\n",
    "        f\"\"\"\n",
    "            This is the CFG file.\n",
    "\n",
    "            # Accuracy\n",
    "            Trn_acc : {self.trn_acc_tr[-1] :.3f}\n",
    "            Test_acc : {self.test_acc_tr[-1] :.3f}\n",
    "\n",
    "            # Loss\n",
    "            Trn_loss : {self.trn_loss_tr[-1].item() :.4f} \n",
    "            Test_loss : {self.test_loss_tr[-1].item() :.4f} \n",
    "\n",
    "\n",
    "            # dataset\n",
    "            dataset = {dataset}\n",
    "            n = {n} # number of total data\n",
    "            d = {d} # dimension\n",
    "            num_class = {num_classes} # the class, used in this binary classification\n",
    "            starting_width = {width}\n",
    "            final_width = {self.width}\n",
    "            # Network\n",
    "            {net}\n",
    "            \n",
    "            # Polytope\n",
    "            number of data used in this training (Class 0 / 1): {class0_inPoly+class0_outPoly} / {class1_inPoly+class1_outPoly}\n",
    "            Class 0 in Polytope : {class0_inPoly}\n",
    "            Class 0 out Polytope : {class0_outPoly}\n",
    "            Class 1 in Polytope : {class1_inPoly}\n",
    "            Class 1 out Polytope : {class1_outPoly}\n",
    "            \n",
    "            # optimization\n",
    "            Epochs = {Epochs}\n",
    "            lr = {lr}\n",
    "            optimizer = {self.optimizer}\n",
    "            Last epoch = {self.epoch+1}\n",
    "            \n",
    "            ## trn_X_index\n",
    "            {trn_X_index}\n",
    "        \"\"\"\n",
    "        )\n",
    "        f.close()\n",
    "    \n",
    "    def pruning(self, dataset):\n",
    "        width_before = self.width\n",
    "        dataset_activation_pattern = self.activation_pattern(dataset).sum(dim=0)\n",
    "        if dataset_activation_pattern.max() == len(dataset) or dataset_activation_pattern.min() == 0 : \n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            activation_filter = ((self.activation_pattern(dataset).sum(dim=0) < len(dataset)).float()\n",
    "                                 * (self.activation_pattern(dataset).sum(dim=0) > 0).float()\n",
    "                                 * (self.W(layer=1).abs()>0.1).float().view(-1)\n",
    "                                )\n",
    "            # change polytope width \n",
    "            self.width = activation_filter.sum().long().item()\n",
    "            width_after = self.width\n",
    "            self.fc0 = nn.Linear(d, self.width)\n",
    "            self.fc1 = nn.Linear(self.width, self.output_class, bias=False)\n",
    "            self.change_layer_weights(layer=0, W=W0[(activation_filter == 1)], b=b0[activation_filter == 1])\n",
    "            self.change_layer_weights(layer=1, W=W1.view(-1)[activation_filter == 1].view(1,-1), b=None)\n",
    "            text = \"(removing)\"\n",
    "            print(f\"\\t{text:<15}The network is prunned, width : {width_before} --> {width_after}\") if pt_option else None\n",
    "            \n",
    "        ## merging vectors with same activation patterns ##\n",
    "        width_activation_pattern = self.activation_pattern(dataset) # len(dataset) x width\n",
    "        width_activation_pattern_unique = width_activation_pattern.unique(dim=1).t()\n",
    "        width_before = self.width\n",
    "        if len(width_activation_pattern_unique) < self.width :\n",
    "            self.width = len(width_activation_pattern_unique) \n",
    "            width_after = self.width\n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            self.fc0 = nn.Linear(d, self.width)\n",
    "            self.fc1 = nn.Linear(self.width, self.output_class, bias=False)\n",
    "            self.to(device)\n",
    "            # build weight matrices\n",
    "            new_W0 = self.W(layer=0).detach()\n",
    "            new_b0 = self.b(layer=0).detach()\n",
    "            new_W1 = self.W(layer=1).detach()\n",
    "            for index, pattern in enumerate(width_activation_pattern_unique):\n",
    "                new_W0[index] = W0[(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "                new_b0[index] = b0[(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "                new_W1[0][index] = W1[0][(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "            self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "            self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "            text = \"(merging)\"\n",
    "            print(f\"\\t{text:<15}The network is merged, width : {width_before} --> {width_after}\") if pt_option else None\n",
    "\n",
    "        ###### original #######\n",
    "        smallest_norm_neuron_index = self.width # not index yet.\n",
    "        smallest_norm = np.inf\n",
    "        width_activation_pattern = self.activation_pattern(dataset) # len(dataset) x width\n",
    "        _count = 0\n",
    "        for neuron_index in range(self.width): # for loop\n",
    "            activated_index = (width_activation_pattern[:, neuron_index] == 1)\n",
    "            # index 가 activated 된 data들의 activation pattern ## 이게 2 이상이면 제거해버리면 됨 !\n",
    "            if self.activation_pattern(dataset[activated_index]).sum(dim=1).min().item() >=2 :\n",
    "                # this neuron can be removed.\n",
    "                _count += 1\n",
    "                this_round_norm = (self.W(layer=0)[neuron_index].norm() * self.W(layer=1).squeeze()[neuron_index]).item() ## v||w||\n",
    "#                 this_round_norm = self.W(layer=1).squeeze()[neuron_index].item() ## v\n",
    "                if this_round_norm < smallest_norm:\n",
    "                    smallest_norm_neuron_index = neuron_index\n",
    "                    smallest_norm = this_round_norm\n",
    "        if smallest_norm_neuron_index < self.width:\n",
    "            # remove the small-norm redundant neuron, remove only one neuron at once\n",
    "            width_before = self.width\n",
    "            width_after = self.width - 1\n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            self.fc0 = nn.Linear(d, width_after)\n",
    "            self.fc1 = nn.Linear(width_after, self.output_class, bias=False)\n",
    "            self.to(device)        \n",
    "            new_W0 = torch.cat((W0[:smallest_norm_neuron_index], W0[smallest_norm_neuron_index+1:]), dim=0)\n",
    "            new_b0 = torch.cat((b0[:smallest_norm_neuron_index], b0[smallest_norm_neuron_index+1:]), dim=0)\n",
    "            new_W1 = torch.cat((W1[:,:smallest_norm_neuron_index], W1[:,smallest_norm_neuron_index+1:]), dim=1)\n",
    "            self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "            self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "            self.width = width_after\n",
    "            text = \"(pruning)\"\n",
    "            print(f\"\\t{text:<15}There are {_count} redundant neurons, and one has been removed\") if pt_option else None\n",
    "        \n",
    "        \n",
    "        ### rescaling function ## here, we don't need to rescale the neuron.\n",
    "        rescale=1.1\n",
    "        intermediate_index = ( ( F.relu(self.forward(dataset)) != 0).float() * (self.forward(dataset) != self.bias).float() >0).squeeze()\n",
    "        if intermediate_index.float().sum() >0 :\n",
    "            activation_pattern_of_intermediate_data = self.activation_pattern(dataset[intermediate_index])\n",
    "            if activation_pattern_of_intermediate_data.max().item() >0 : ## max\n",
    "                new_W0 = self.W(layer=0).detach()\n",
    "                new_b0 = self.b(layer=0).detach()\n",
    "                new_W1 = self.W(layer=1).detach()\n",
    "                true_or_false = activation_pattern_of_intermediate_data.sum(dim=0)>0\n",
    "                for i, boolean in enumerate(true_or_false):\n",
    "                    if boolean:\n",
    "                        new_W0[i] = new_W0[i] * rescale\n",
    "                        new_b0[i] = new_b0[i] * rescale\n",
    "                        new_W1[0][i] = new_W1[0][i] * rescale\n",
    "                self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "                self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "                text = \"(rescaling)\"\n",
    "                print(f\"\\t{text:<15}In the polytope, {true_or_false.sum().item()} neurons are rescaled by {rescale}\") if pt_option else None\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr) # net. paramgeter 를 optimizer 가능하도록 넣어줘야해...!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27268242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are finding polytope-basis cover of class [1] of MNIST, n=60000, with random ratio=0.0, with acc 99.99\n",
      "width_fail: []   ||    width_success: []\n",
      "Now width = 40\n",
      "=================================   Start Training. Repetition: 1   ==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▏                                                                | 12484/100000 [01:50<12:51, 113.40it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimization setting\n",
    "lr = 0.0001\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# finding the polytope cover !\n",
    "repetition = 0\n",
    "trn_X_index_all = (torch.ones(len(trn_X))>0).to(device) # Boolean tensor, shape = len(dataset)\n",
    "trn_X_index = trn_X_index_all.clone()\n",
    "# folder_name_above = output_path + f'/runs/{dataset}/' + datetime.datetime.now().strftime(\"%B%d_%H_%M_%S\")\n",
    "feature = \"c_\" if first_positive_init else \"_\"\n",
    "folder_name_above = output_path + f'/runs/{dataset}/' + f\"{selected_class}{feature}\"\n",
    "create_directory(folder_name_above) # create output folder\n",
    "network_list =[]\n",
    "\n",
    "complement = \"^c\" if first_positive_init else \"\"\n",
    "print(f\"We are finding polytope-basis cover of class [{selected_class}]\"+complement+f\" of {dataset}, n={n}, with random ratio={random_ratio}, with acc {Goal_Accuracy}\")\n",
    "positive_init = first_positive_init\n",
    "# while trn_X_index[(trn_Y==0).view(-1)].sum()>0 and trn_X_index[(trn_Y==1).view(-1)].sum()>0 : # 100% accuracy\n",
    "width_fail = list()\n",
    "width_success = list()\n",
    "width_difference = 1\n",
    "# width_increment = int(input(\"### Please enter the width_increment: ###\")) ## manually adjust the width\n",
    "\n",
    "for _count in range(15):\n",
    "    # set width\n",
    "    if len(width_success) == 0 :\n",
    "        width += width_increment # increment\n",
    "    elif len(width_fail) == 0 :\n",
    "        width = int (np.array(width_success).min()/2)\n",
    "    else: \n",
    "        width = int( (np.array(width_fail).max() + np.array(width_success).min())/2 + torch.randint(min(width_difference , 10)+3, (1,)).item()/2 ) # intermediate value\n",
    "#     width = int(input(\"### Please enter the network width: ###\")) ################ manually adjust the width\n",
    "    \n",
    "    print(\"width_fail:\", width_fail, \"  ||    width_success:\", width_success)\n",
    "    print(f\"Now width = {width}\")\n",
    "    net = polytope(width=width, positive_init=positive_init, small_norm_init=False).to(device)\n",
    "    folder_name = folder_name_above + f\"/Cover_{repetition+1}\"\n",
    "    writer = SummaryWriter(folder_name)\n",
    "    \n",
    "    # training\n",
    "    trn_X_index_this_time = trn_X_index.clone()\n",
    "    ####\n",
    "#     trn_X_index_this_time[(trn_Y==float(positive_init)).view(-1)] = True  #### consider all data points in the other class\n",
    "    ####\n",
    "    net.train(repetition=repetition, trn_X_index= trn_X_index_this_time)\n",
    "#     again = input(\"### Do you wanna train again? (y/n) ###\")\n",
    "#     if again == 'y':\n",
    "#         net.fail = True\n",
    "    if net.fail:\n",
    "        # there is some problem for finding a cover in this training time.\n",
    "        width_fail.append(net.width)\n",
    "    else : # success !\n",
    "            # otherwise, it was successful.\n",
    "        # result\n",
    "        print(\"Completed to find a polytope-basis cover.\")\n",
    "        print(net)\n",
    "        width_success.append(net.width)\n",
    "        network_list = []\n",
    "        network_list.append(net) # add to the network_list\n",
    "    repetition += 1\n",
    "    \n",
    "    if (len(width_fail) != 0) and (len(width_success) != 0) :\n",
    "        width_difference = (np.array(width_success).min() - np .array(width_fail).max())\n",
    "        if width_difference <= 1 :\n",
    "            print(\"Found a single polytope cover\")\n",
    "            print(\"width_fail :\", width_fail)\n",
    "            print(\"width_success :\", width_success)\n",
    "            break\n",
    "print(\"\\n\\n\\n===== Found covers =====\")\n",
    "print(\"width_fail :\", width_fail)\n",
    "print(\"width_success :\", width_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae85314",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Result ####\n",
    "width_list = [net.width for net in network_list]\n",
    "print(f\"Output: a polytope-basis cover of class [{selected_class}]{complement} with random ratio {random_ratio}\")\n",
    "print(f\"There are totally {len(network_list)} polytopes. The widths are\",width_list,end=\"\\n\\n\")\n",
    "\n",
    "#### Accuracy #########\n",
    "# trn acc\n",
    "previous_trn_answer = torch.zeros_like(trn_Y)\n",
    "for j, net in enumerate(network_list):\n",
    "    previous_trn_answer += (-1)**j * (net(trn_X) == net.bias).float()\n",
    "correct_trn_data = ((previous_trn_answer>0).float() == float(first_positive_init) + (1-2*(float(first_positive_init)))*trn_Y).sum().item()\n",
    "trn_acc =  correct_trn_data / len(trn_Y) * 100\n",
    "print(f\"Trn ACC : {trn_acc:.2f} %  ||  # incorrect data : {len(trn_Y) - correct_trn_data} / {len(trn_Y)}\")\n",
    "\n",
    "# test ACC\n",
    "previous_test_answer = torch.zeros_like(test_Y)\n",
    "for j, net in enumerate(network_list):\n",
    "    previous_test_answer += (-1)**j * (net(test_X) == net.bias).float()\n",
    "correct_test_data = ((previous_test_answer>0).float() == float(first_positive_init) + (1-2*(float(first_positive_init)))*test_Y).sum().item()\n",
    "test_acc =  correct_test_data / len(test_Y) * 100\n",
    "print(f\"Test ACC : {test_acc:.2f} %  ||  # incorrect data : {len(test_Y) - correct_test_data} / {len(test_Y)}\")\n",
    "\n",
    "f = open(f\"./{folder_name_above}/Total_result.txt\", 'w')\n",
    "f.write(\n",
    "f\"\"\"\n",
    "    This is the result file\n",
    "    \n",
    "    # dataset\n",
    "    dataset = {dataset}, {dataset_type}\n",
    "    n = {len(trn_X)} # number of trained data\n",
    "    d = {d} # dimension\n",
    "    Initialization width = {width}\n",
    "    data_balance = {data_balance}\n",
    "    \n",
    "    # class, convexity\n",
    "    selected_class = {selected_class} # the class, used in this binary classification\n",
    "    positive_init = {first_positive_init}\n",
    "    Therefore, it finds a polytope-basis cover of class [{selected_class}]{complement})\n",
    "    random ratio = {random_ratio}\n",
    "    class_overlapped = {class_overlapped}\n",
    "    \n",
    "    width_fail : {width_fail}\n",
    "    width_success : {width_success}\n",
    "    \n",
    "    # Accuracy\n",
    "    Trn ACC : {trn_acc:.2f} %  ||  # incorrect data : {len(trn_Y) - correct_trn_data} / {len(trn_Y)}\n",
    "    Test ACC : {test_acc:.2f} %  ||  # incorrect data : {len(test_Y) - correct_test_data} / {len(test_Y)}\n",
    "    \n",
    "    # optimization\n",
    "    Epochs = {Epochs}\n",
    "    Start_pruning = {start_pruning}\n",
    "    \n",
    "    There are totally {len(network_list)} polytopes. The widths are\n",
    "    {width_list}\n",
    "    \n",
    "    The end.\n",
    "\"\"\"\n",
    ")\n",
    "f.close()\n",
    "\n",
    "print(\"\\nwidth_fail :\", width_fail)\n",
    "print(\"width_success :\", width_success)\n",
    "## rename the folder name.\n",
    "if len(width_success)>0:\n",
    "    os.rename(folder_name_above, folder_name_above + str(np.array(width_success).min()))\n",
    "else:\n",
    "    os.rename(folder_name_above, folder_name_above + \"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e71c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### For visualization ######\n",
    "# for x,y in zip(trn_X, trn_Y):\n",
    "# #     if net(x) == net.bias and y != 1:\n",
    "#     if net(x) == net.bias and y != 1:\n",
    "# #     if y == 1:\n",
    "#         imshow(x.cpu().view(28,28), title=y.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
