{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a85041",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e23521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.patches as mpatches\n",
    "# plt.rcParams['text.usetex'] = True\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.facecolor': \"None\"})\n",
    "rcParams.update({'savefig.transparent': True})\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "rcParams.update({'figure.dpi': 200})\n",
    "rcParams.update({\"mathtext.fontset\" : \"cm\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d67b7",
   "metadata": {},
   "source": [
    "Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42820418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image show function\n",
    "def imshow(x, title=None, figsize=(2,2)):\n",
    "    if torch.is_tensor(x) :\n",
    "        _x = x.detach().cpu().squeeze()\n",
    "        plt.figure(figsize=figsize)\n",
    "        if _x.dim() == 2:\n",
    "            plt.imshow(_x, cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(_x)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    else :\n",
    "        plt.imshow(x.squeeze())\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        \n",
    "def create_directory(directory): \n",
    "    try: \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError: print(\"Error: Failed to create the directory.\")\n",
    "        \n",
    "def savefig(directory):\n",
    "    plt.savefig(directory)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def protractor(w,v):\n",
    "    # function that calculates angle between two torch tensors\n",
    "    return torch.arccos((w*v).sum()/(w.norm()*v.norm()))\n",
    "\n",
    "def make_list_of_width(depth, width, dimension):\n",
    "    list_of_width = []\n",
    "    list_of_width.append(dimension)\n",
    "    for _depth in range(depth-1):\n",
    "        list_of_width.append(width)\n",
    "    list_of_width.append(dimension)\n",
    "    return list_of_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset, n):\n",
    "    if dataset == \"MNIST\" :\n",
    "        trn_dataset = torchvision.datasets.MNIST(\"./DATASETS\", train=True, download=True,\n",
    "                                                         transform=torchvision.transforms.Compose(\n",
    "                                                             [torchvision.transforms.ToTensor(),\n",
    "                                                              torchvision.transforms.Normalize((0.1307,), (0.3081,)) # MNIST\n",
    "                                                             ]\n",
    "                                                         ))\n",
    "        test_dataset = torchvision.datasets.MNIST(\"./DATASETS\", train=False, download=True, \n",
    "                                                         transform=torchvision.transforms.Compose(\n",
    "                                                             [torchvision.transforms.ToTensor(),\n",
    "                                                              torchvision.transforms.Normalize((0.1307,), (0.3081,)) # MNIST\n",
    "                                                             ]\n",
    "                                                         ))\n",
    "    elif dataset == \"FashionMNIST\":\n",
    "        trn_dataset = torchvision.datasets.FashionMNIST(\"./DATASETS\", train=True, download=True,\n",
    "                                                         transform=torchvision.transforms.Compose(\n",
    "                                                             [torchvision.transforms.ToTensor(),\n",
    "                                                              torchvision.transforms.Normalize((0.2860,), (0.3530,)) # Fashion MNIST\n",
    "                                                             ]\n",
    "                                                         ))\n",
    "        test_dataset = torchvision.datasets.FashionMNIST(\"./DATASETS\", train=False, download=True, \n",
    "                                                         transform=torchvision.transforms.Compose(\n",
    "                                                             [torchvision.transforms.ToTensor(),\n",
    "                                                              torchvision.transforms.Normalize((0.2860,), (0.3530,)) # Fashion MNIST\n",
    "                                                             ]\n",
    "                                                         ))\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        trn_dataset = torchvision.datasets.CIFAR10(\"./DATASETS\", train=True, download=True,\n",
    "                                                         transform=torchvision.transforms.Compose(\n",
    "                                                             [torchvision.transforms.ToTensor(),\n",
    "                                                              torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)) # CIFAR10\n",
    "                                                             ]\n",
    "                                                         ))\n",
    "        test_dataset = torchvision.datasets.CIFAR10(\"./DATASETS\", train=False, download=True, \n",
    "                                                         transform=torchvision.transforms.Compose(\n",
    "                                                             [torchvision.transforms.ToTensor(),\n",
    "                                                              torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)) # CIFAR10\n",
    "                                                             ]\n",
    "                                                         ))\n",
    "    else:\n",
    "        raise ValueError(\"Dataset name is unvalid.\")\n",
    "    \n",
    "    \n",
    "    trn_data_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=n, shuffle=False, num_workers=0, pin_memory=False, drop_last=True) # do not shuffle.\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10000, shuffle=False, num_workers=0, pin_memory=False, drop_last=True)\n",
    "    \n",
    "    \n",
    "    for data in trn_data_loader:\n",
    "        trn_X, trn_Y = data[0], data[1]\n",
    "        break\n",
    "    trn_X = trn_X.view(-1, d)\n",
    "    trn_Y = trn_Y.float()\n",
    "\n",
    "    for data in test_data_loader:\n",
    "        test_X, test_Y = data[0], data[1]\n",
    "    test_X = test_X.view(-1, d)\n",
    "    test_Y = test_Y.float()\n",
    "\n",
    "    # trn_X = trn_X.view(n,-1).to(device) # FC net\n",
    "    trn_X = trn_X.to(device)\n",
    "    trn_Y = trn_Y.to(device)\n",
    "    # test_X = test_X.view(10000,-1).to(device) # FC net\n",
    "    test_X = test_X.to(device)\n",
    "    test_Y = test_Y.to(device)\n",
    "    return trn_X, trn_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95443efc",
   "metadata": {},
   "source": [
    "Nework Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03deb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Network architectures\n",
    "# class two_layer_ReLU_net(nn.Module):\n",
    "#     def __init__(self, d1, output_class=2, bias=True, last_ReLU=True):\n",
    "#         super(two_layer_ReLU_net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(d, d1, bias=bias)\n",
    "#         self.fc2 = nn.Linear(d1, output_class, bias=bias)\n",
    "#         self.d1 = d1 # width\n",
    "#         self.last_ReLU = last_ReLU\n",
    "#         self.output_class = output_class\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.g1 = self.fc1(x)\n",
    "#         self.h1 = F.relu(self.g1)\n",
    "#         self.g2 = self.fc2(self.h1)\n",
    "#         self.h2 = F.relu(self.g2)\n",
    "#         if self.last_ReLU:\n",
    "#             return self.h2\n",
    "#         else:\n",
    "#             return self.g2\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "#         self.forward(x) # set h1 and h2\n",
    "#         return self.h1>0 # binary tensor\n",
    "    \n",
    "    \n",
    "#     def rank(self, x, e=0.001): # compute the rank at x\n",
    "#         diff = self.forward(x+e) - self.forward(x)\n",
    "#         rank = (diff != 0).sum(-1)\n",
    "#         return rank\n",
    "    \n",
    "#     def partition(self, X, title, save_location, show_data=True, rank_measure='output rank', show_vector=False):\n",
    "#         w = 2\n",
    "#         N = 400\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "#         if rank_measure == 'decision boundary':\n",
    "#             rank = (net(grid.view(-1,2))<0).float().view(N,N)                     # decision boundary, output>0\n",
    "#             plt.contourf(x,y,rank.cpu(), np.arange(-1,2,1), cmap='gray')\n",
    "# #             plt.contourf(x,y,rank.cpu(), np.arange(-1,2,0.1), cmap='RdBu_r')\n",
    "#         elif rank_measure == 'output rank':\n",
    "#             rank = net.output_rank(grid.view(-1,2)).view(N,N)                     # Output rank\n",
    "#             plt.contourf(x,y,rank.cpu(), np.arange(0,self.output_class+1,1), cmap='gray')\n",
    "#         elif rank_measure == 'function rank':\n",
    "#             rank = net.function_rank(grid.view(-1,2)).view(N,N)                 # Function rank\n",
    "#             if self.last_ReLU:\n",
    "#                 plt.contourf(x,y,rank.cpu(), np.arange(0,self.output_class+1,1), cmap='gray')\n",
    "#             else:\n",
    "#                 plt.contourf(x,y,rank.cpu(), np.arange(0,self.m+1,1), cmap='gray')\n",
    "#         elif rank_measure == 'matrix rank':\n",
    "#             rank = net.matrix_rank(grid.view(-1,2)).view(N,N)                 # Function rank\n",
    "#             plt.contourf(x,y,rank.cpu(), np.arange(0,4 ,1), cmap='gray')\n",
    "#         else:\n",
    "#             raise RankMeasureError\n",
    "#         plt.colorbar()\n",
    "        \n",
    "#         if self.last_ReLU :\n",
    "#             activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+output_class)\n",
    "#         else:\n",
    "#             activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1)\n",
    "        \n",
    "#         for i in range(self.d1):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#         if self.last_ReLU:\n",
    "#             for i in range(self.d1,self.d1+self.output_class):\n",
    "#                 plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='yellow', linewidths=0.5)\n",
    "\n",
    "#         black_patch = mpatches.Patch(color='black', label='Rank 0')\n",
    "#         gray_patch = mpatches.Patch(color='gray', label='Rank 1')\n",
    "#         blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#         plt.legend(handles=[blue_patch])\n",
    "        \n",
    "#         if show_vector:\n",
    "#             _x, _y = torch.meshgrid(torch.linspace(-w,w,20), torch.linspace(-w,w,20))\n",
    "#             _grid = torch.stack((_x,_y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "#             out = net(_grid.view(-1,2))\n",
    "#             _u = out[:,0]-_grid.view(-1,2)[:,0]\n",
    "#             _v = out[:,1]-_grid.view(-1,2)[:,1]\n",
    "#             plt.quiver(_x.cpu(),_y.cpu(),_u.detach().cpu(),_v.detach().cpu())\n",
    "        \n",
    "#         plt.plot(trn_X[trn_Y.squeeze()>0.5][:,0].cpu(), trn_X[trn_Y.squeeze()>0.5][:,1].cpu(), '*g')\n",
    "#         if show_data :\n",
    "#             # plot dataset\n",
    "#             plt.plot(trn_X[trn_Y.squeeze()<0.5][:,0].cpu(), trn_X[trn_Y.squeeze()<0.5][:,1].cpu(), '*k')\n",
    "#         plt.xlabel('x')\n",
    "#         plt.ylabel('y')\n",
    "#         plt.title(f\"Net:2->{self.d1}->{self.output_class}, \"+title)\n",
    "#         plt.xlim(-w,w)\n",
    "#         plt.ylim(-w,w)\n",
    "#         plt.savefig(save_location)\n",
    "# #         plt.show()\n",
    "#         plt.clf()\n",
    "\n",
    "    \n",
    "    \n",
    "# ###############################################################################\n",
    "# class three_layer_ReLU_net(nn.Module):\n",
    "#     def __init__(self, d1, d2, output_class=2, bias=True, last_ReLU=True):\n",
    "#         super(three_layer_ReLU_net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(d, d1, bias=bias)\n",
    "#         self.fc2 = nn.Linear(d1, d2, bias=bias)\n",
    "#         self.fc3 = nn.Linear(d2, output_class, bias=bias)\n",
    "#         self.d1 = d1\n",
    "#         self.d2 = d2\n",
    "#         self.last_ReLU = last_ReLU\n",
    "#         self.output_class = output_class\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.g1 = self.fc1(x)\n",
    "#         self.h1 = F.relu(self.g1)\n",
    "#         self.g2 = self.fc2(self.h1)\n",
    "#         self.h2 = F.relu(self.g2)\n",
    "#         self.g3 = self.fc3(self.h2)\n",
    "#         self.h3 = F.relu(self.g3)\n",
    "#         if self.last_ReLU:\n",
    "#             return self.h3\n",
    "#         else:\n",
    "#             return self.g3\n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "#         self.forward(x) # set h1 and h2\n",
    "#         if self.last_ReLU :\n",
    "#             return torch.cat((self.h1>0, self.h2>0, self.h3>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "#         else:\n",
    "#             return torch.cat((self.h1>0, self.h2>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "    \n",
    "    \n",
    "#     def output_rank(self, x, e=0.001): # compute the output rank at x\n",
    "#         diff = self.forward(x+e) - self.forward(x)\n",
    "#         output_rank = (diff != 0).sum(-1)\n",
    "#         return output_rank\n",
    "    \n",
    "#     def function_rank(self, x): # compute the function rank at x\n",
    "#         self.forward(x)\n",
    "#         if self.last_ReLU:\n",
    "#             function_rank = torch.min(torch.stack(((self.h1!=0).sum(1), (self.h2!=0).sum(1), (self.h3!=0).sum(1)), dim=0), dim=0)[0]\n",
    "#         else:\n",
    "#             function_rank = torch.min(torch.stack(((self.h1!=0).sum(1), (self.h2!=0).sum(1)), dim=0), dim=0)[0]\n",
    "#         return function_rank#.clamp(0,2)\n",
    "    \n",
    "#     def matrix_rank(self, x): # compute the matrix rank at x // the matrix A s.t. N(x) = Ax + b\n",
    "#         batchsize = len(x)\n",
    "#         A = torch.cat(\n",
    "#             (self.forward(x + torch.tensor([1.0, 0.0]).to(device)) - self.forward(x),\n",
    "#              self.forward(x + torch.tensor([0.0, 1.0]).to(device)) - self.forward(x)), dim=0\n",
    "#         ) # shape 2batchsize x output_class\n",
    "#         z = torch.zeros(int(len(A)/2))\n",
    "#         for i in range(len(z)): ########################################################## for loop is too ineffective ################\n",
    "#             ############################################ How can I fix it effectively ?? ###############################################\n",
    "#             z[i] = torch.linalg.matrix_rank(A[2*i:2*i+2]).item()\n",
    "#         return z\n",
    "        \n",
    "    \n",
    "#     def partition(self, X, title, save_location, loss, w=2, only_data=False, showfig=False):\n",
    "# #         w = 20\n",
    "#         N = 400\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        \n",
    "#         if only_data:\n",
    "#             plt.plot(X[:,0].cpu(), X[:,1].cpu(), '*g')\n",
    "#             plt.xlabel('x')\n",
    "#             plt.ylabel('y')\n",
    "#             plt.title(\"Dataset: two triangles\")\n",
    "#             plt.xlim(-w,w)\n",
    "#             plt.ylim(-w,w)\n",
    "#             plt.savefig(save_location)\n",
    "#             plt.clf()\n",
    "            \n",
    "#         else:\n",
    "#             rank = net(grid.view(-1,2)).view(N,N).detach()                     # decision boundary, output>0\n",
    "#             if loss == 'MSE':\n",
    "#                 plt.contourf(x,y,rank.cpu(), np.array([-0.4,0,0.4, 0.8,1.2]), cmap='Greys') #### MSE\n",
    "#             elif loss == 'BCE':\n",
    "#                 plt.contourf(x,y,torch.sigmoid(rank.cpu()), np.array([-0,0.5,1]), cmap='Greys') #### BCE\n",
    "#             plt.colorbar()\n",
    "\n",
    "#             if self.last_ReLU :\n",
    "#                 activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+self.d2+output_class)\n",
    "#                 for i in range(self.d1):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#                 for i in range(self.d1,self.d1+self.d2):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "#                 blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#                 red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#                 plt.legend(handles=[blue_patch, red_patch])\n",
    "#             else:\n",
    "#                 activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+self.d2)\n",
    "#                 for i in range(self.d1):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#                 for i in range(self.d1,self.d1+self.d2):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "#                 blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#                 red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#                 plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "#             plt.plot(trn_X[trn_Y.squeeze()>0.5][:,0].cpu(), trn_X[trn_Y.squeeze()>0.5][:,1].cpu(), '*g')\n",
    "#             plt.xlabel('x')\n",
    "#             plt.ylabel('y')\n",
    "#             if loss == 'MSE':\n",
    "#                 plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                       +r\"$\\longrightarrow$\"+f\"{self.output_class}, \"+title)\n",
    "#             elif loss==\"BCE\":\n",
    "#                 plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\mathtt{SIG}}{\\longrightarrow}$\"+f\"{self.output_class}, \"+title)\n",
    "#             plt.xlim(-w,w)\n",
    "#             plt.ylim(-w,w)\n",
    "#             plt.savefig(save_location)\n",
    "#             if showfig:\n",
    "#                 plt.show()\n",
    "#             plt.clf()\n",
    "\n",
    "    \n",
    "    \n",
    "# ###############################################################################\n",
    "# class four_layer_ReLU_net(nn.Module):\n",
    "#     def __init__(self, d1, d2, d3, output_class=1, bias=True, last_ReLU=False):\n",
    "#         super(four_layer_ReLU_net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(d, d1, bias=bias)\n",
    "#         self.fc2 = nn.Linear(d1, d2, bias=bias)\n",
    "#         self.fc3 = nn.Linear(d2, d3, bias=bias)\n",
    "#         self.fc4 = nn.Linear(d3, output_class, bias=bias)\n",
    "#         self.d1 = d1 # width\n",
    "#         self.d2 = d2 # width\n",
    "#         self.d3 = d3 # width\n",
    "#         self.last_ReLU = last_ReLU\n",
    "#         self.output_class = output_class\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.g1 = self.fc1(x)\n",
    "#         self.h1 = F.relu(self.g1)\n",
    "#         self.g2 = self.fc2(self.h1)\n",
    "#         self.h2 = F.relu(self.g2)\n",
    "#         self.g3 = self.fc3(self.h2)\n",
    "#         self.h3 = F.relu(self.g3)\n",
    "#         self.g4 = self.fc4(self.h3)\n",
    "#         self.h4 = F.relu(self.g4)\n",
    "#         if self.last_ReLU:\n",
    "#             return self.h4\n",
    "#         else:\n",
    "#             return self.g4\n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "#         self.forward(x) # set h1 and h2\n",
    "#         if self.last_ReLU :\n",
    "#             return torch.cat((self.h1>0, self.h2>0, self.h3>0, self.h4>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "#         else:\n",
    "#             return torch.cat((self.h1>0, self.h2>0, self.h3>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "    \n",
    "#     def partition(self, X, title, save_location, loss, only_data=False, showfig=False):\n",
    "#         w = 2\n",
    "#         N = 400\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        \n",
    "#         if only_data:\n",
    "#             plt.plot(X[:,0].cpu(), X[:,1].cpu(), '*g')\n",
    "#             plt.xlabel('x')\n",
    "#             plt.ylabel('y')\n",
    "#             plt.title(\"Dataset: two triangles\")\n",
    "#             plt.xlim(-w,w)\n",
    "#             plt.ylim(-w,w)\n",
    "#             plt.savefig(save_location)\n",
    "#             plt.clf()\n",
    "            \n",
    "#         else:\n",
    "#             rank = net(grid.view(-1,2)).view(N,N).detach()                     # 400 x 400 x 2 tensor\n",
    "#             if loss == 'MSE':\n",
    "#                 plt.contourf(x,y,rank.cpu(), np.array([-0.4,0,0.4, 0.8,1.2]), cmap='Greys', extend='both') #### MSE\n",
    "#             elif loss == 'BCE':\n",
    "#                 plt.contourf(x,y,torch.sigmoid(rank.cpu()), np.array([-0,0.5,1]), cmap='Greys') #### BCE\n",
    "#             plt.colorbar()\n",
    "            \n",
    "#             if self.last_ReLU :\n",
    "#                 activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+self.d2+self.d3+output_class)\n",
    "#                 for i in range(self.d1):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#                 for i in range(self.d1,self.d1+self.d2):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "#                 for i in range(self.d1+self.d2,self.d1+self.d2+self.d3):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='yellow', linewidths=0.5)\n",
    "#                 for i in range(self.d1+self.d2+self.d3, self.d1+self.d2+self.d3+output_class):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='purple', linewidths=0.5)\n",
    "\n",
    "#                 blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#                 red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#                 yellow_patch = mpatches.Patch(color='yellow', label='3rd layer')\n",
    "#                 purple_patch = mpatches.Patch(color='purple', label='4th layer')\n",
    "#                 plt.legend(handles=[blue_patch, red_patch, yellow_patch, purple_patch])\n",
    "#             else:\n",
    "#                 activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+self.d2+self.d3+output_class)\n",
    "#                 for i in range(self.d1):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#                 for i in range(self.d1,self.d1+self.d2):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "#                 for i in range(self.d1+self.d2,self.d1+self.d2+self.d3):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='yellow', linewidths=0.5)\n",
    "#                 for i in range(self.d1+self.d2+self.d3, self.d1+self.d2+self.d3+output_class):\n",
    "#                     plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='purple', linewidths=0.5)\n",
    "                    \n",
    "#                 blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#                 red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#                 yellow_patch = mpatches.Patch(color='yellow', label='3rd layer')\n",
    "#                 plt.legend(handles=[blue_patch, red_patch, yellow_patch])\n",
    "\n",
    "#             plt.plot(trn_X[trn_Y.squeeze()>0.5][:,0].cpu(), trn_X[trn_Y.squeeze()>0.5][:,1].cpu(), '*g')\n",
    "#             plt.xlabel('x')\n",
    "#             plt.ylabel('y')\n",
    "#             if loss == 'MSE':\n",
    "#                 plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d3}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.output_class}, \"+title)\n",
    "#             elif loss==\"BCE\":\n",
    "#                 plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d3}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\mathtt{SIG}}{\\longrightarrow}$\"+f\"{self.output_class}, \"+title)\n",
    "#             plt.xlim(-w,w)\n",
    "#             plt.ylim(-w,w)\n",
    "#             plt.savefig(save_location)\n",
    "#             if showfig:\n",
    "#                 plt.show()\n",
    "#             plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be643e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ###############################################################################    \n",
    "# class five_layer_ReLU_net(nn.Module):\n",
    "#     def __init__(self, d1,d2,d3,d4,output_class=1, bias=True, last_ReLU=False):\n",
    "#         super(five_layer_ReLU_net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(d, d1, bias=bias)\n",
    "#         self.fc2 = nn.Linear(d1, d2, bias=bias)\n",
    "#         self.fc3 = nn.Linear(d2, d3, bias=bias)\n",
    "#         self.fc4 = nn.Linear(d3, d4, bias=bias)\n",
    "#         self.fc5 = nn.Linear(d4, output_class, bias=bias)\n",
    "#         self.d1 = d1\n",
    "#         self.d2 = d2\n",
    "#         self.d3 = d3\n",
    "#         self.d4 = d4\n",
    "#         self.last_ReLU = last_ReLU\n",
    "#         self.output_class = output_class\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.g1 = self.fc1(x)\n",
    "#         self.h1 = F.relu(self.g1)\n",
    "#         self.g2 = self.fc2(self.h1)\n",
    "#         self.h2 = F.relu(self.g2)\n",
    "#         self.g3 = self.fc3(self.h2)\n",
    "#         self.h3 = F.relu(self.g3)\n",
    "#         self.g4 = self.fc4(self.h3)\n",
    "#         self.h4 = F.relu(self.g4)\n",
    "#         self.g5 = self.fc5(self.h4)\n",
    "#         self.h5 = F.relu(self.g5)\n",
    "#         if self.last_ReLU:\n",
    "#             return self.h5\n",
    "#         else:\n",
    "#             return self.g5\n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "#         self.forward(x) # set h1 and h2\n",
    "#         if self.last_ReLU :\n",
    "#             return torch.cat((self.h1>0, self.h2>0, self.h3>0, self.h4>0, self.h5>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "#         else:\n",
    "#             return torch.cat((self.h1>0, self.h2>0, self.h3>0, self.h4>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "    \n",
    "#     def partition(self, X, title, save_location, loss, show_data=True, show_vector=False):\n",
    "#         w = 2\n",
    "#         N = 400\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "#         rank = net(grid.view(-1,2)).view(N,N).detach()                     # decision boundary, output>0\n",
    "#         if loss == 'MSE':\n",
    "#             plt.contourf(x,y,rank.cpu(), np.array([-0.4,0,0.4, 0.8,1.2]), cmap='Greys', extend='both') #### MSE\n",
    "#         elif loss == 'BCE':\n",
    "#             plt.contourf(x,y,torch.sigmoid(rank.cpu()), np.array([-0,0.5,1]), cmap='Greys') #### BCE\n",
    "#         plt.colorbar()\n",
    "        \n",
    "#         if self.last_ReLU :\n",
    "#             activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+self.d2+self.d3+self.d4+output_class)\n",
    "#         else:\n",
    "#             activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+self.d2+self.d3+self.d4)\n",
    "        \n",
    "#         for i in range(self.d1):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#         for i in range(self.d1,self.d1+self.d2):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "#         for i in range(self.d1+self.d2,self.d1+self.d2+self.d3):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='yellow', linewidths=0.5)\n",
    "#         for i in range(self.d1+self.d2+self.d3, self.d1+self.d2+self.d3+self.d4):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='purple', linewidths=0.5)\n",
    "#         if self.last_ReLU:\n",
    "#             for i in range(self.d1+self.d2+self.d3+self.d4,self.d1+self.d2+self.d3+self.d4+self.output_class):\n",
    "#                 plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='orange', linewidths=0.5)\n",
    "\n",
    "#         black_patch = mpatches.Patch(color='black', label='Rank 0')\n",
    "#         gray_patch = mpatches.Patch(color='gray', label='Rank 1')\n",
    "#         blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#         red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#         yellow_patch = mpatches.Patch(color='yellow', label='3rd layer')\n",
    "#         purple_patch = mpatches.Patch(color='purple', label='4th layer')\n",
    "#         orange_patch = mpatches.Patch(color='orange', label='5th layer')\n",
    "#         plt.legend(handles=[blue_patch, red_patch, yellow_patch, purple_patch])\n",
    "        \n",
    "#         if show_data :\n",
    "#             # plot dataset\n",
    "# #             plt.plot(trn_X[trn_Y.squeeze()>0.5][:,0].cpu(), trn_X[trn_Y.squeeze()>0][:,1].cpu(), '*g')\n",
    "# #             plt.plot(trn_X[trn_Y.squeeze()<0.5][:,0].cpu(), trn_X[trn_Y.squeeze()<0][:,1].cpu(), '.k')\n",
    "#             plt.plot(trn_X[trn_Y.squeeze()>0.5][:,0].cpu(), trn_X[trn_Y.squeeze()>0.5][:,1].cpu(), '*g')\n",
    "            \n",
    "#         plt.xlabel('x')\n",
    "#         plt.ylabel('y')\n",
    "#         plt.title(f\"Net:2->{self.d1}->{self.d2}->{self.output_class}, \"+title)\n",
    "#         plt.xlim(-w,w)\n",
    "#         plt.ylim(-w,w)\n",
    "#         plt.savefig(save_location)\n",
    "# #         plt.show()\n",
    "#         plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245e359",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class three_layer_MAX_net(nn.Module):\n",
    "#     def __init__(self, d1, d2, output_class=1, bias=True, last_ReLU=False):\n",
    "#         super(three_layer_MAX_net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(d, d1, bias=bias)\n",
    "#         self.fc2 = nn.Linear(d1, d2, bias=bias)\n",
    "#         self.d1 = d1\n",
    "#         self.d2 = d2\n",
    "#         self.last_ReLU = last_ReLU\n",
    "#         self.output_class = output_class\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.g1 = self.fc1(x)\n",
    "#         self.h1 = F.relu(self.g1)\n",
    "#         self.g2 = self.fc2(self.h1)\n",
    "#         self.h2 = F.relu(self.g2)\n",
    "#         self.g3 = self.h2.max(dim=1)[0]\n",
    "#         return self.g3.view(-1,1)\n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "#         self.forward(x) # set h1 and h2\n",
    "#         return torch.cat((self.h1>0, self.h2>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "    \n",
    "#     def partition(self, X, title, save_location, loss, only_data=False, showfig=False):\n",
    "#         w = 20\n",
    "#         N = 400\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        \n",
    "#         if only_data:\n",
    "#             plt.plot(X[:,0].cpu(), X[:,1].cpu(), '*g')\n",
    "#             plt.xlabel('x')\n",
    "#             plt.ylabel('y')\n",
    "#             plt.title(\"Dataset: two triangles\")\n",
    "#             plt.xlim(-w,w)\n",
    "#             plt.ylim(-w,w)\n",
    "#             plt.savefig(save_location)\n",
    "#             plt.clf()\n",
    "            \n",
    "#         else:\n",
    "#             rank = net(grid.view(-1,2)).view(N,N).detach()                     # decision boundary, output>0\n",
    "#             if loss == 'MSE':\n",
    "#                 plt.contourf(x,y,rank.cpu(), np.array([-0.4,0,0.4, 0.8, 1.2]), cmap='Greys', extend='both') #### MSE\n",
    "#             elif loss == 'BCE':\n",
    "#                 plt.contourf(x,y,torch.sigmoid(rank.cpu()), np.array([-0,0.5,1]), cmap='Greys') #### BCE\n",
    "#             plt.colorbar()\n",
    "\n",
    "#             activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1 + self.d2)\n",
    "\n",
    "#             for i in range(self.d1):\n",
    "#                 plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#             for i in range(self.d1, self.d2):\n",
    "#                 plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "\n",
    "#             blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#             red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#             plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "#             plt.plot(trn_X[trn_Y.squeeze()>0.5][:,0].cpu(), trn_X[trn_Y.squeeze()>0.5][:,1].cpu(), '*g')\n",
    "#             plt.xlabel('x')\n",
    "#             plt.ylabel('y')\n",
    "#             if loss == 'MSE':\n",
    "#                 plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\mathtt{MAX}}{\\longrightarrow}$\"+f\"{self.output_class}, \"+title)\n",
    "#             elif loss==\"BCE\":\n",
    "#                 plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\mathtt{BCE}}{\\longrightarrow}$\"+f\"{self.output_class}, \"+title)\n",
    "#             plt.xlim(-w,w)\n",
    "#             plt.ylim(-w,w)\n",
    "#             plt.savefig(save_location)\n",
    "#             if showfig:\n",
    "#                 plt.show()\n",
    "#             plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "# class three_layer_MAX_two_blocks(nn.Module):\n",
    "#     def __init__(self, d1, d2, output_class=1, bias=True):\n",
    "#         super(three_layer_MAX_two_blocks, self).__init__()\n",
    "#         self.fc11 = nn.Linear(d, d+1, bias=bias)\n",
    "#         self.fc12 = nn.Linear(d+1, 1, bias=bias)\n",
    "#         self.fc21 = nn.Linear(d, d+1, bias=bias)\n",
    "#         self.fc22 = nn.Linear(d+1, 1, bias=bias)\n",
    "#         self.d1 = d1\n",
    "#         self.d2 = d2\n",
    "#         self.output_class = output_class\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.g11 = self.fc11(x)\n",
    "#         self.h11 = F.relu(self.g11)\n",
    "#         self.g12 = self.fc12(self.h11)\n",
    "#         self.h12 = F.relu(self.g12)\n",
    "        \n",
    "#         self.g21 = self.fc21(x)\n",
    "#         self.h21 = F.relu(self.g21)\n",
    "#         self.g22 = self.fc22(self.h21)\n",
    "#         self.h22 = F.relu(self.g22)\n",
    "        \n",
    "#         self.g3 = torch.max(self.h12, self.h22)\n",
    "#         return self.g3\n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "#         self.forward(x) # set h1 and h2\n",
    "#         return torch.cat((self.h11>0,self.h21>0, self.h12>0,self.h22>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "    \n",
    "#     def partition(self, X, title, save_location, loss, only_data=False):\n",
    "#         w = 20\n",
    "#         N = 400\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        \n",
    "#         if only_data:\n",
    "#             plt.plot(X[:,0].cpu(), X[:,1].cpu(), '*g')\n",
    "#             plt.xlabel('x')\n",
    "#             plt.ylabel('y')\n",
    "#             plt.title(\"Dataset: two triangles\")\n",
    "#             plt.xlim(-w,w)\n",
    "#             plt.ylim(-w,w)\n",
    "#             plt.savefig(save_location)\n",
    "#             plt.clf()\n",
    "            \n",
    "#         else:\n",
    "#             rank = net(grid.view(-1,2)).view(N,N).detach()                     # decision boundary, output>0\n",
    "#             if loss == 'MSE':\n",
    "#                 plt.contourf(x,y,rank.cpu(), np.array([-0.4,0,0.4, 0.8,1.2]), cmap='Greys') #### MSE\n",
    "#             elif loss == 'BCE':\n",
    "#                 plt.contourf(x,y,F.sigmoid(rank.cpu()), np.array([-0,0.5,1]), cmap='Greys') #### BCE\n",
    "#             plt.colorbar()\n",
    "\n",
    "#             activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,2*(d+1)+2)\n",
    "\n",
    "#             for i in range(2*(d+1)):\n",
    "#                 plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#             for i in range(2*(d+1), 2*(d+1)+2):\n",
    "#                 plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "\n",
    "#             blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#             red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#             plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "#             plt.plot(trn_X[trn_Y.squeeze()>0.5][:,0].cpu(), trn_X[trn_Y.squeeze()>0.5][:,1].cpu(), '*g')\n",
    "#             plt.xlabel('x')\n",
    "#             plt.ylabel('y')\n",
    "#             if loss == 'MSE':\n",
    "#                 plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\mathtt{MAX}}{\\longrightarrow}$\"+f\"{self.output_class}, \"+title)\n",
    "#             elif loss==\"BCE\":\n",
    "#                 plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                       +r\"$\\genfrac{}{}{0}{}{\\mathtt{SIG}}{\\longrightarrow}$\"+f\"{self.output_class}, \"+title)\n",
    "#             plt.xlim(-w,w)\n",
    "#             plt.ylim(-w,w)\n",
    "#             plt.savefig(save_location)\n",
    "#             plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dee87a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class three_layer_MAX_six_blocks(nn.Module):\n",
    "#     def __init__(self, d1, d2, output_class=1, bias=True):\n",
    "#         super(three_layer_MAX_six_blocks, self).__init__()\n",
    "#         self.fc11 = nn.Linear(d, d+1, bias=bias)\n",
    "#         self.fc12 = nn.Linear(d+1, 1, bias=bias)\n",
    "#         self.fc21 = nn.Linear(d, d+1, bias=bias)\n",
    "#         self.fc22 = nn.Linear(d+1, 1, bias=bias)\n",
    "#         self.fc31 = nn.Linear(d, d+1, bias=bias)\n",
    "#         self.fc32 = nn.Linear(d+1, 1, bias=bias)\n",
    "#         self.fc41 = nn.Linear(d, d+1, bias=bias)\n",
    "#         self.fc42 = nn.Linear(d+1, 1, bias=bias)\n",
    "#         self.fc51 = nn.Linear(d, d+1, bias=bias)\n",
    "#         self.fc52 = nn.Linear(d+1, 1, bias=bias)\n",
    "#         self.fc61 = nn.Linear(d, d+1, bias=bias)\n",
    "#         self.fc62 = nn.Linear(d+1, 1, bias=bias)\n",
    "#         self.d1 = d1\n",
    "#         self.d2 = d2\n",
    "#         self.output_class = output_class\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.g11 = self.fc11(x)\n",
    "#         self.h11 = F.relu(self.g11)\n",
    "#         self.g12 = self.fc12(self.h11)\n",
    "#         self.h12 = F.relu(self.g12)\n",
    "        \n",
    "#         self.g21 = self.fc21(x)\n",
    "#         self.h21 = F.relu(self.g21)\n",
    "#         self.g22 = self.fc22(self.h21)\n",
    "#         self.h22 = F.relu(self.g22)\n",
    "        \n",
    "#         self.g31 = self.fc31(x)\n",
    "#         self.h31 = F.relu(self.g31)\n",
    "#         self.g32 = self.fc22(self.h31)\n",
    "#         self.h32 = F.relu(self.g32)\n",
    "        \n",
    "#         self.g41 = self.fc41(x)\n",
    "#         self.h41 = F.relu(self.g41)\n",
    "#         self.g42 = self.fc22(self.h41)\n",
    "#         self.h42 = F.relu(self.g42)\n",
    "        \n",
    "#         self.g51 = self.fc51(x)\n",
    "#         self.h51 = F.relu(self.g51)\n",
    "#         self.g52 = self.fc22(self.h51)\n",
    "#         self.h52 = F.relu(self.g52)\n",
    "        \n",
    "#         self.g61 = self.fc61(x)\n",
    "#         self.h61 = F.relu(self.g61)\n",
    "#         self.g62 = self.fc22(self.h61)\n",
    "#         self.h62 = F.relu(self.g62)\n",
    "        \n",
    "#         self.g3 = torch.max( torch.cat( (self.h12, self.h22, self.h32, self.h42,self.h52, self.h62), dim=1 )\n",
    "#                             , dim=1)[0].view(-1,1)\n",
    "#         return self.g3\n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "#         self.forward(x) # set h1 and h2\n",
    "#         # binary tensor, shape width+width+output_class\n",
    "#         return torch.cat((self.h11>0, self.h21>0, self.h31>0, self.h41>0, self.h51>0, self.h61>0,\n",
    "#                           self.h12>0, self.h22>0, self.h32>0, self.h42>0, self.h52>0, self.h62>0), dim=1) \n",
    "    \n",
    "#     def partition(self, X, title, save_location, rank_measure='output rank'):\n",
    "#         w = 20\n",
    "#         N = 400\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "#         if rank_measure == 'decision boundary':\n",
    "#             rank = net(grid.view(-1,2)).view(N,N).detach()                     # decision boundary, output>0\n",
    "#             plt.contourf(x,y,rank.cpu()/10, np.arange(-1,3,1), cmap='Greys')\n",
    "#         plt.colorbar()\n",
    "        \n",
    "#         activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,6*(d+2))\n",
    "        \n",
    "#         for i in range(6*(d+1)):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#         for i in range(6*(d+1), 6*(d+1)+6):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "\n",
    "#         blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#         red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#         plt.legend(handles=[blue_patch, red_patch])\n",
    "        \n",
    "#         plt.plot(trn_X[trn_Y.squeeze()>0.5][:,0].cpu(), trn_X[trn_Y.squeeze()>0.5][:,1].cpu(), '*g')\n",
    "#         plt.xlabel('x')\n",
    "#         plt.ylabel('y')\n",
    "#         plt.title(\"Network:  \"+r\"$2\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d1}\"\n",
    "#                   +r\"$\\genfrac{}{}{0}{}{\\sigma}{\\longrightarrow}$\"+f\"{self.d2}\"\n",
    "#                   +r\"$\\genfrac{}{}{0}{}{\\mathtt{MAX}}{\\longrightarrow}$\"+f\"{self.output_class}, \"+title)\n",
    "#         plt.xlim(-w,w)\n",
    "#         plt.ylim(-w,w)\n",
    "#         plt.savefig(save_location)\n",
    "#         plt.clf()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# class four_layer_conv_net(nn.Module):\n",
    "#     def __init__(self, d1, d2, d3, output_class=1, bias=True, last_ReLU=False):\n",
    "#         super(four_layer_conv_net, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(1, d1, kernel_size=2, bias=bias)\n",
    "#         self.fc2 = nn.Linear(d1, d2, bias=bias)\n",
    "#         self.fc3 = nn.Linear(d2, d3, bias=bias)\n",
    "#         self.fc4 = nn.Linear(d3, output_class, bias=bias)\n",
    "#         self.d1 = d1 # width\n",
    "#         self.d2 = d2 # width\n",
    "#         self.d3 = d3 # width\n",
    "#         self.last_ReLU = last_ReLU\n",
    "#         self.output_class = output_class\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.g1 = self.conv1(x.view(-1,1,2)) # one channel\n",
    "#         self.h1 = F.relu(self.g1).view(-1,self.d1)\n",
    "#         self.g2 = self.fc2(self.h1)\n",
    "#         self.h2 = F.relu(self.g2)\n",
    "#         self.g3 = self.fc3(self.h2)\n",
    "#         self.h3 = F.relu(self.g3)\n",
    "#         self.g4 = self.fc4(self.h3)\n",
    "#         self.h4 = F.relu(self.g4)\n",
    "#         if self.last_ReLU:\n",
    "#             return self.h4\n",
    "#         else:\n",
    "#             return self.g4\n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "#         self.forward(x) # set h1 and h2\n",
    "#         if self.last_ReLU :\n",
    "#             return torch.cat((self.h1>0, self.h2>0, self.h3>0, self.h4>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "#         else:\n",
    "#             return torch.cat((self.h1>0, self.h2>0, self.h3>0), dim=1) # binary tensor, shape width+width+output_class\n",
    "    \n",
    "    \n",
    "#     def output_rank(self, x, e=0.001): # compute the output rank at x\n",
    "#         diff = self.forward(x+e) - self.forward(x)\n",
    "#         output_rank = (diff != 0).sum(-1)\n",
    "#         return output_rank\n",
    "    \n",
    "#     def function_rank(self, x): # compute the function rank at x\n",
    "#         self.forward(x)\n",
    "#         if self.last_ReLU:\n",
    "#             function_rank = torch.min(torch.stack(((self.h1!=0).sum(1), (self.h2!=0).sum(1), (self.h3!=0).sum(1), (self.h4!=0).sum(1)), dim=0), dim=0)[0]\n",
    "#         else:\n",
    "#             function_rank = torch.min(torch.stack(((self.h1!=0).sum(1), (self.h2!=0).sum(1), (self.h3!=0).sum(1)), dim=0), dim=0)[0]\n",
    "#         return function_rank\n",
    "    \n",
    "#     def partition(self, X, title, save_location, show_data=True, rank_measure='output rank', show_vector=False):\n",
    "#         w = 20\n",
    "#         N = 400\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "#         if rank_measure == 'decision boundary':\n",
    "#             rank = net(grid.view(-1,2)).view(N,N).detach()                     # decision boundary, output>0\n",
    "#             plt.contourf(x,y,rank.cpu(), np.arange(-5,15,5), cmap='Greys')\n",
    "# #             plt.contourf(x,y,rank.cpu(), np.arange(-1,2,0.1), cmap='RdBu_r')\n",
    "#         elif rank_measure == 'output rank':\n",
    "#             rank = net.output_rank(grid.view(-1,2)).view(N,N)                     # Output rank\n",
    "#             plt.contourf(x,y,rank.cpu(), np.arange(0,self.output_class+1,1), cmap='gray')\n",
    "#         elif rank_measure == 'function rank':\n",
    "#             rank = net.function_rank(grid.view(-1,2)).view(N,N)                 # Function rank\n",
    "#             plt.contourf(x,y,rank.cpu(), np.arange(0,self.m+1,1), cmap='gray')\n",
    "#         else:\n",
    "#             raise RankMeasureError\n",
    "#         plt.colorbar()\n",
    "        \n",
    "#         if self.last_ReLU :\n",
    "#             activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+self.d2+self.d3+output_class)\n",
    "#         else:\n",
    "#             activation_pattern = net.activation_pattern(grid.view(-1,2)).view(N,N,self.d1+self.d2+self.d3)\n",
    "        \n",
    "#         for i in range(self.d1):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#         for i in range(self.d1,self.d1+self.d2):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "#         for i in range(self.d1+self.d2,self.d1+self.d2+self.d3):\n",
    "#             plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='yellow', linewidths=0.5)\n",
    "#         if self.last_ReLU:\n",
    "#             for i in range(self.d1+self.d2+self.d3,self.d1+self.d2+self.d3+self.output_class):\n",
    "#                 plt.contour(x,y,activation_pattern.float().cpu()[:,:,i], levels=1, colors='purple', linewidths=0.5)\n",
    "\n",
    "#         black_patch = mpatches.Patch(color='black', label='Rank 0')\n",
    "#         gray_patch = mpatches.Patch(color='gray', label='Rank 1')\n",
    "#         blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#         red_patch = mpatches.Patch(color='red', label='2nd layer')\n",
    "#         yellow_patch = mpatches.Patch(color='yellow', label='3rd layer')\n",
    "#         purple_patch = mpatches.Patch(color='purple', label='4th layer')\n",
    "#         plt.legend(handles=[blue_patch, red_patch, yellow_patch])\n",
    "        \n",
    "#         if show_data :\n",
    "#             # plot dataset\n",
    "#             plt.plot(X[:,0].cpu(), X[:,1].cpu(), '*g')\n",
    "#         plt.xlabel('x')\n",
    "#         plt.ylabel('y')\n",
    "#         plt.title(f\"ConvNet:2->{self.d1}->{self.d2}->{self.d3}->{self.output_class}, \"+title)\n",
    "#         plt.xlim(-w,w)\n",
    "#         plt.ylim(-w,w)\n",
    "#         plt.savefig(save_location)\n",
    "# #         plt.show()\n",
    "#         plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b2efb",
   "metadata": {},
   "source": [
    "Polytope-basis cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa94086e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Network setting # d -> d1 -> 1\n",
    "class polytope(nn.Module):\n",
    "    def __init__(self, width, output_class=1, positive_init=True):\n",
    "        super(polytope, self).__init__()\n",
    "        self.fc0 = nn.Linear(d, width)\n",
    "        self.fc1 = nn.Linear(width, output_class, bias=False)\n",
    "        self.width = width # width\n",
    "        self.bias = 1 - 2*positive_init\n",
    "        \n",
    "        if positive_init:\n",
    "            # initialization, to all v_k are negative.\n",
    "            self.fc1.weight = nn.Parameter((self.W(0).norm(dim=1)**2 + self.b(0)**2).view(1,-1)+1)\n",
    "        else:\n",
    "            # initialization, to all v_k are negative.\n",
    "            self.fc1.weight = nn.Parameter(-(self.W(0).norm(dim=1)**2 + self.b(0)**2).view(1,-1)-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.g1 = self.fc0(x)\n",
    "        self.h1 = F.relu(self.g1)\n",
    "        self.g2 = self.fc1(self.h1) + self.bias\n",
    "        return self.g2\n",
    "    \n",
    "    def W(self, i):\n",
    "        if i ==0:\n",
    "            output = self.fc0.weight\n",
    "        elif i ==1:\n",
    "            output = self.fc1.weight\n",
    "        return output\n",
    "    def b(self, i):\n",
    "        if i ==0:\n",
    "            output = self.fc0.bias\n",
    "        elif i ==1:\n",
    "            output = self.fc1.bias\n",
    "        return output\n",
    "    \n",
    "    def activation_pattern(self, x):\n",
    "        self.forward(x)\n",
    "        return (self.h1>0).float()\n",
    "    \n",
    "    def change_layer_weights(self, i, W, b):\n",
    "        if W.shape == self.W(i).shape and b.shape == self.b(i).shape:\n",
    "            if i ==0:\n",
    "                self.fc0.weight = nn.Parameter(W)\n",
    "                self.fc0.bias = nn.Parameter(b)\n",
    "            elif i ==1:\n",
    "                self.fc1.weight = nn.Parameter(W)\n",
    "                self.fc1.bias = nn.Parameter(b)\n",
    "        else:\n",
    "            raise ValueError(\"wrong shape of input tensors\")\n",
    "    \n",
    "    def partition(self, w=16):\n",
    "        N = 200\n",
    "        x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "        grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        rank = (self.forward(grid.view(-1,2))<0).float().view(N,N)                     # decision boundary, output<0\n",
    "        plt.contourf(x,y,rank.cpu(), np.arange(-1,2,1), cmap='gray')\n",
    "        #     plt.contourf(x,y,rank.cpu(), np.arange(-1,2,0.1), cmap='RdBu_r')\n",
    "        plt.colorbar()\n",
    "        for i in range(self.width):\n",
    "            plt.contour(x,y, self.activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "\n",
    "        black_patch = mpatches.Patch(color='black', label='Rank 0')\n",
    "        gray_patch = mpatches.Patch(color='gray', label='Rank 1')\n",
    "        blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "        plt.legend(handles=[blue_patch])\n",
    "        # dataset\n",
    "        plt.scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.')\n",
    "        plt.scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.')\n",
    "        plt.title(\"Activation and decision boundary\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7bc54b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# multiple polytopes : polytope basis-cover\n",
    "class cover(nn.Module):\n",
    "    def __init__(self, width, output_class=1, positive_init=True):\n",
    "        super(cover, self).__init__()\n",
    "        self.width=width\n",
    "        self.polytope_A1 = polytope(width=width, positive_init=positive_init)\n",
    "        self.polytope_B1 = polytope(width=width, positive_init=positive_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.a1 = self.polytope_A1(x)\n",
    "        self.b1 = self.polytope_B1(x)\n",
    "\n",
    "#         output = (2*(F.relu(self.a1) > F.relu(self.b1)).float()-1) * torch.max(self.a1, self.b1)\n",
    "        if positive_init:\n",
    "            output = torch.min(self.a1, self.b1)\n",
    "        else:\n",
    "            output = torch.max(self.a1, self.b1)\n",
    "        return output\n",
    "    \n",
    "    def activation_pattern(self, x):\n",
    "#         self.forward(x)\n",
    "        output = torch.cat((self.polytope_A1.activation_pattern(x), self.polytope_B1.activation_pattern(x)))\n",
    "        return output\n",
    "    \n",
    "    def all_deactivated(self, x):\n",
    "        output = (self.polytope_A1.activation_pattern(trn_X).sum(dim=1)==0).float()\n",
    "        output += (self.polytope_B1.activation_pattern(trn_X).sum(dim=1)==0).float()\n",
    "        return (output>0).sum().item()\n",
    "    \n",
    "    def partition(self, w=2):\n",
    "        N = 200\n",
    "        x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "        grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        rank = (net(grid.view(-1,2))<0).float().view(N,N)                     # decision boundary, output<0\n",
    "        plt.contourf(x,y,rank.cpu(), np.arange(-1,2,1), cmap='gray')\n",
    "        #     plt.contourf(x,y,rank.cpu(), np.arange(-1,2,0.1), cmap='RdBu_r')\n",
    "        plt.colorbar()\n",
    "\n",
    "        for i in range(net.width):\n",
    "            plt.contour(x,y, net.polytope_A1.activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "            plt.contour(x,y, net.polytope_B1.activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "\n",
    "        black_patch = mpatches.Patch(color='black', label='Rank 0')\n",
    "        gray_patch = mpatches.Patch(color='gray', label='Rank 1')\n",
    "        blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "        plt.legend(handles=[blue_patch])\n",
    "        # dataset\n",
    "        plt.scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.')\n",
    "        plt.scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.')\n",
    "        plt.title(\"Activation and decision boundary\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41796057",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # multiple polytopes : polytope basis-cover\n",
    "# class cover5(nn.Module):\n",
    "#     def __init__(self, width, num_polytopes, output_class=1, positive_init=True):\n",
    "#         super(cover5, self).__init__()\n",
    "#         self.width=width\n",
    "#         self.num_polytopes=num_polytopes\n",
    "#         self.polytope_A1 = polytope(width=width, positive_init=positive_init)\n",
    "#         self.polytope_A2 = polytope(width=width, positive_init=positive_init)\n",
    "#         self.polytope_A3 = polytope(width=width, positive_init=positive_init)\n",
    "#         self.polytope_A4 = polytope(width=width, positive_init=positive_init)\n",
    "#         self.polytope_A5 = polytope(width=width, positive_init=positive_init)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.a1 = self.polytope_A1(x)\n",
    "#         self.a2 = self.polytope_A1(x)\n",
    "#         self.a3 = self.polytope_A1(x)\n",
    "#         self.a4 = self.polytope_A1(x)\n",
    "#         self.a5 = self.polytope_A1(x)\n",
    "        \n",
    "#         output = torch.cat((self.a1, self.a2, self.a3, self.a4, self.a5), dim=1)\n",
    "\n",
    "# #         output = (2*(F.relu(self.a1) > F.relu(self.b1)).float()-1) * torch.max(self.a1, self.b1)\n",
    "#         if positive_init:\n",
    "#             output = torch.min(output, dim=1)[0]\n",
    "#         else:\n",
    "#             output = torch.max(output, dim=1)[0]\n",
    "#         return output\n",
    "    \n",
    "#     def activation_pattern(self, x):\n",
    "# #         self.forward(x)\n",
    "#         output = torch.cat((self.polytope_A1.activation_pattern(x), \n",
    "#                             self.polytope_A2.activation_pattern(x),\n",
    "#                             self.polytope_A3.activation_pattern(x),\n",
    "#                             self.polytope_A4.activation_pattern(x),\n",
    "#                             self.polytope_A5.activation_pattern(x)\n",
    "#                            ), dim=1)\n",
    "#         return output\n",
    "    \n",
    "#     def all_deactivated(self, x):\n",
    "#         output = (self.polytope_A1.activation_pattern(trn_X).sum(dim=1)==0).float()\n",
    "#         output += (self.polytope_A2.activation_pattern(trn_X).sum(dim=1)==0).float()\n",
    "#         output += (self.polytope_A3.activation_pattern(trn_X).sum(dim=1)==0).float()\n",
    "#         output += (self.polytope_A4.activation_pattern(trn_X).sum(dim=1)==0).float()\n",
    "#         output += (self.polytope_A5.activation_pattern(trn_X).sum(dim=1)==0).float()\n",
    "#         return (output>0).sum().item()\n",
    "    \n",
    "#     def partition(self, w=16):\n",
    "#         N = 200\n",
    "#         x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "#         grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "#         rank = (net(grid.view(-1,2))<0).float().view(N,N)                     # decision boundary, output<0\n",
    "#         plt.contourf(x,y,rank.cpu(), np.arange(-1,2,1), cmap='gray')\n",
    "#         #     plt.contourf(x,y,rank.cpu(), np.arange(-1,2,0.1), cmap='RdBu_r')\n",
    "#         plt.colorbar()\n",
    "\n",
    "#         for i in range(net.width):\n",
    "#             plt.contour(x,y, net.polytope_A1.activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "#             plt.contour(x,y, net.polytope_B1.activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "\n",
    "#         black_patch = mpatches.Patch(color='black', label='Rank 0')\n",
    "#         gray_patch = mpatches.Patch(color='gray', label='Rank 1')\n",
    "#         blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#         plt.legend(handles=[blue_patch])\n",
    "#         # dataset\n",
    "#         plt.scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.')\n",
    "#         plt.scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.')\n",
    "#         plt.title(\"Activation and decision boundary\")\n",
    "#         plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
