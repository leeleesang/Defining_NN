{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e33ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification, Intrinsic dimension, Degeneracy\n",
    "%run SetUp.ipynb\n",
    "%run synthetic_datasets.ipynb\n",
    "\n",
    "## Configuration \n",
    "Title = \"Synthetic_datasets\" # general two layer\n",
    "device = \"cuda:1\"\n",
    "pt_option=True\n",
    "col = 1 # for visualization, figure 에 column 을 얼마로 할건지 ?\n",
    "num_polytopes = (1*col, 1*col)\n",
    "width = 8\n",
    "dataset = \"Two circles\" # Swiss / Two circles / Two moons / XOR\n",
    "d = 2\n",
    "n = 1000 # used number of data\n",
    "num_classes = 2\n",
    "\n",
    "## load datasets ##\n",
    "trn_X, trn_Y = load_synthetic_dataset(dataset)\n",
    "Epochs = 150*1000\n",
    "start_pruning = 10000\n",
    "pruning_period = 2000\n",
    "lambda_rescale = 1.2\n",
    "positive_init = False\n",
    "small_norm_init = True\n",
    "\n",
    "# create directory\n",
    "output_path = f\"./Results/{Title}\"\n",
    "create_directory(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f57f1",
   "metadata": {},
   "source": [
    "Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520d8d1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Network setting # d -> d1 -> 1\n",
    "class polytope(nn.Module):\n",
    "    def __init__(self, width, output_class=1, positive_init=True, small_norm_init=True):\n",
    "        super(polytope, self).__init__()\n",
    "        self.output_class = output_class\n",
    "        self.fc0 = nn.Linear(d, width)\n",
    "        self.fc1 = nn.Linear(width, output_class, bias=False)\n",
    "        self.width = width # width\n",
    "        self.bias = (1 - 2*positive_init)\n",
    "        self.positive_init = positive_init\n",
    "        self.output_class = output_class\n",
    "        self.small_norm_init = small_norm_init\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "        if positive_init:\n",
    "            # initialization, to all v_k are negative.\n",
    "            self.fc1.weight = nn.Parameter(  torch.sqrt((self.W(layer=0).norm(dim=1)**2 + self.b(0)**2+1).view(1,-1))  )\n",
    "        else:\n",
    "            # initialization, to all v_k are negative.\n",
    "            self.fc1.weight = nn.Parameter(  -torch.sqrt((self.W(layer=0).norm(dim=1)**2 + self.b(0)**2+1).view(1,-1))  )\n",
    "        \n",
    "        if small_norm_init:\n",
    "            self.change_layer_weights(layer=0, W=1.5/self.width*torch.randn_like(self.W(0)), b=0.6/self.width*(self.b(0))) # multiply 0.01 \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.g1 = self.fc0(x)\n",
    "        self.h1 = F.relu(self.g1)\n",
    "        self.g2 = self.fc1(self.h1) + self.bias\n",
    "        return self.g2\n",
    "    \n",
    "    def W(self, layer):\n",
    "        if layer ==0:\n",
    "            output = self.fc0.weight\n",
    "        elif layer ==1:\n",
    "            output = self.fc1.weight\n",
    "        return output.clone()\n",
    "    def b(self, layer):\n",
    "        if layer ==0:\n",
    "            output = self.fc0.bias\n",
    "        elif layer ==1:\n",
    "            output = self.fc1.bias\n",
    "        return output if output == None else output.clone()\n",
    "    \n",
    "    def activation_pattern(self, x):\n",
    "        self.forward(x)\n",
    "        return (self.h1>0).float()\n",
    "    \n",
    "    def change_layer_weights(self, layer, W, b):\n",
    "        if self.b(layer) == None : # no bias term\n",
    "            if W.shape == self.W(layer).shape and self.b(layer) == b:\n",
    "                if layer ==0:\n",
    "                    self.fc0.weight = nn.Parameter(W.to(self.device))\n",
    "                elif layer ==1:\n",
    "                    self.fc1.weight = nn.Parameter(W.to(self.device))\n",
    "            else:\n",
    "                raise ValueError(\"wrong shape of input tensors\")\n",
    "        else:\n",
    "            if W.shape == self.W(layer).shape and b.shape == self.b(layer).shape:\n",
    "                if layer ==0:\n",
    "                    self.fc0.weight = nn.Parameter(W.to(self.device))\n",
    "                    self.fc0.bias = nn.Parameter(b.to(self.device))\n",
    "                elif layer ==1:\n",
    "                    self.fc1.weight = nn.Parameter(W.to(self.device))\n",
    "    #                 self.fc1.bias = nn.Parameter(b)\n",
    "            else:\n",
    "                raise ValueError(\"wrong shape of input tensors\")\n",
    "    \n",
    "    def partition(self, w=1.6, title=\"img\"):\n",
    "        N = 200\n",
    "        x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "        grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        rank = (self.forward(grid.view(-1,2))<0.001).float().view(N,N)                     # decision boundary, output<0\n",
    "        plt.contourf(x,y,rank.cpu(), np.arange(-1,2,1), cmap='gray')\n",
    "        #     plt.contourf(x,y,rank.cpu(), np.arange(-1,2,0.1), cmap='RdBu_r')\n",
    "        plt.colorbar()\n",
    "        for i in range(self.width):\n",
    "            plt.contour(x,y, self.activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "\n",
    "#         black_patch = mpatches.Patch(color='black', label='Rank 0')\n",
    "#         gray_patch = mpatches.Patch(color='gray', label='Rank 1')\n",
    "#         blue_patch = mpatches.Patch(color='blue', label='1st layer')\n",
    "#         plt.legend(handles=[blue_patch])\n",
    "        # dataset\n",
    "        plt.scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.')\n",
    "        plt.scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.')\n",
    "        plt.title(fr\"Polytope $P_1$; width={self.width}\")\n",
    "        plt.savefig(folder_name+f\"/{title}.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def pruning(self, subnetwork_index=0):\n",
    "        dataset = trn_X # the dataset.\n",
    "        \n",
    "        ## remove a neuron which deactivates all data. # STEP 1\n",
    "        width_before = self.width\n",
    "        dataset_activation_pattern = self.activation_pattern(dataset).sum(dim=0)\n",
    "        if dataset_activation_pattern.max() == len(dataset) or dataset_activation_pattern.min() == 0 :  # either all or none of data points.\n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            activation_filter = ((self.activation_pattern(dataset).sum(dim=0) < len(dataset)).float()\n",
    "                                 * (self.activation_pattern(dataset).sum(dim=0) > 0).float()\n",
    "                                 * (self.W(layer=1).abs()>0.1).float().view(-1)\n",
    "                                )\n",
    "            # change polytope width \n",
    "            self.width = activation_filter.sum().long().item()\n",
    "            if self.width > 0 :\n",
    "                width_after = self.width\n",
    "                self.fc0 = nn.Linear(d, self.width)\n",
    "                self.fc1 = nn.Linear(self.width, self.output_class, bias=False)\n",
    "                self.change_layer_weights(layer=0, W=W0[(activation_filter == 1)], b=b0[activation_filter == 1])\n",
    "                self.change_layer_weights(layer=1, W=W1.view(-1)[activation_filter == 1].view(1,-1), b=None)\n",
    "                text = \"(removing)\"\n",
    "                print(f\"\\t{text:<15}The subnetwork {subnetwork_index} is prunned, width : {width_before} --> {width_after}\") if pt_option else None\n",
    "            else:\n",
    "                text = \"(removing)\"\n",
    "                print(f\"\\t{text:<15}The subnetwork {subnetwork_index} has been completely prunned,\") if pt_option else None\n",
    "            \n",
    "        ## merging vectors with the same activation patterns ## # STEP 2\n",
    "        width_activation_pattern = self.activation_pattern(dataset) # len(dataset) x width\n",
    "        width_activation_pattern_unique = width_activation_pattern.unique(dim=1).t()\n",
    "        width_before = self.width\n",
    "        if len(width_activation_pattern_unique) < self.width :\n",
    "            self.width = len(width_activation_pattern_unique) \n",
    "            width_after = self.width\n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            self.fc0 = nn.Linear(d, self.width)\n",
    "            self.fc1 = nn.Linear(self.width, self.output_class, bias=False)\n",
    "            self.to(device)\n",
    "            # build weight matrices\n",
    "            new_W0 = self.W(layer=0).detach()\n",
    "            new_b0 = self.b(layer=0).detach()\n",
    "            new_W1 = self.W(layer=1).detach()\n",
    "            for index, pattern in enumerate(width_activation_pattern_unique):\n",
    "                new_W0[index] = W0[(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "                new_b0[index] = b0[(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "                new_W1[0][index] = W1[0][(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "            self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "            self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "            text = \"(merging)\"\n",
    "            print(f\"\\t{text:<15}The subnetwork {subnetwork_index} is merged, width : {width_before} --> {width_after}\") if pt_option else None\n",
    "            \n",
    "            \n",
    "        ###### original ####### # STEP 3\n",
    "        smallest_norm_neuron_index = self.width # not index yet.\n",
    "        smallest_norm = np.inf\n",
    "        width_activation_pattern = self.activation_pattern(dataset) # len(dataset) x width\n",
    "        _count = 0\n",
    "        for neuron_index in range(self.width): # for loop\n",
    "            activated_index = (width_activation_pattern[:, neuron_index] == 1)\n",
    "            # index 가 activated 된 data들의 activation pattern ## 이게 2 이상이면 제거해버리면 됨 !\n",
    "            if self.activation_pattern(dataset[activated_index]).sum(dim=1).min().item() >=2 : # 항상 다른게 activate 되어있다.\n",
    "                # this neuron can be removed.\n",
    "                _count += 1\n",
    "                this_round_norm = (self.W(layer=0)[neuron_index].norm() * self.W(layer=1).squeeze()[neuron_index]).item() ## v||w||\n",
    "#                 this_round_norm = self.W(layer=1).squeeze()[neuron_index].item() ## v\n",
    "                if this_round_norm < smallest_norm:\n",
    "                    smallest_norm_neuron_index = neuron_index\n",
    "                    smallest_norm = this_round_norm\n",
    "        if smallest_norm_neuron_index < self.width:\n",
    "            # remove the small-norm redundant neuron, remove only one neuron at once\n",
    "            width_before = self.width\n",
    "            width_after = self.width - 1\n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            self.fc0 = nn.Linear(d, width_after)\n",
    "            self.fc1 = nn.Linear(width_after, self.output_class, bias=False)\n",
    "            self.to(device)        \n",
    "            new_W0 = torch.cat((W0[:smallest_norm_neuron_index], W0[smallest_norm_neuron_index+1:]), dim=0)\n",
    "            new_b0 = torch.cat((b0[:smallest_norm_neuron_index], b0[smallest_norm_neuron_index+1:]), dim=0)\n",
    "            new_W1 = torch.cat((W1[:,:smallest_norm_neuron_index], W1[:,smallest_norm_neuron_index+1:]), dim=1)\n",
    "            self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "            self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "            self.width = width_after\n",
    "            text = \"(pruning)\"\n",
    "            print(f\"\\t{text:<15}In the subnetwork {subnetwork_index}, there are {_count} redundant neurons, and one has been removed\") if pt_option else None\n",
    "                \n",
    "\n",
    "    def rescaling_the_boundary_neurons(self, subnetwork_index=0, rescale=1.1): # STEP 4\n",
    "        dataset= trn_X\n",
    "        intermediate_index = ( ( F.relu(self.forward(trn_X)) != 0).float() * (self.forward(trn_X) != self.bias).float() >0).squeeze()\n",
    "        if intermediate_index.float().sum() >0 :\n",
    "            activation_pattern_of_intermediate_data = self.activation_pattern(trn_X[intermediate_index])\n",
    "            if activation_pattern_of_intermediate_data.max().item() >0 : ## max\n",
    "                new_W0 = self.W(layer=0).detach()\n",
    "                new_b0 = self.b(layer=0).detach()\n",
    "                new_W1 = self.W(layer=1).detach()\n",
    "                true_or_false = activation_pattern_of_intermediate_data.sum(dim=0)>0\n",
    "                for i, boolean in enumerate(true_or_false):\n",
    "                    if boolean:\n",
    "                        new_W0[i] = new_W0[i] * rescale\n",
    "                        new_b0[i] = new_b0[i] * rescale\n",
    "                        new_W1[0][i] = new_W1[0][i] * rescale\n",
    "                self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "                self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "                text = \"(rescaling)\"\n",
    "                print(f\"\\t{text:<15}In the subnetwork {subnetwork_index}, {true_or_false.sum().item()} neurons are rescaled by {rescale}\") if pt_option else None\n",
    "                \n",
    "    \n",
    "    def convexity(self):\n",
    "        convexity = (self.W(1).sgn() == self.bias).float().sum() # if convex, this is zero. either one.\n",
    "        return convexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55839db4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# multiple polytopes : polytope basis-cover\n",
    "class cover(nn.Module):\n",
    "    def __init__(self, width, num_polytopes, output_class=1, positive_init=True, small_norm_init=True):\n",
    "        super(cover, self).__init__()\n",
    "        assert len(num_polytopes) == 2, \"must be a tuple of two integers\"\n",
    "        self.num_polytopes = num_polytopes # (+_polytope, -_polytope)\n",
    "        self.width = width\n",
    "        self.device = device\n",
    "        self.module_list = nn.ModuleList() # list of polytopes\n",
    "        for layer in range(num_polytopes[0] + num_polytopes[1]):\n",
    "            self.module_list.append(polytope(width=width, positive_init=positive_init, small_norm_init=small_norm_init))\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, x, partial_sum=None):\n",
    "        positive_output = 0\n",
    "        for layer in range(num_polytopes[0]): \n",
    "            positive_output += F.relu(self.module_list[layer](x))\n",
    "        negative_output = 0\n",
    "        for layer in range(num_polytopes[0], num_polytopes[0]+num_polytopes[1]):  # negative polytope\n",
    "            negative_output += F.relu(self.module_list[layer](x))\n",
    "        if partial_sum == 'positive':\n",
    "            output = positive_output\n",
    "        elif partial_sum == 'negative':\n",
    "            output = negative_output\n",
    "        else:\n",
    "            output = positive_output - negative_output ## (+polytopes) - (-polytopes) #### three-layer network architecture.\n",
    "        return output\n",
    "    \n",
    "    def activation_pattern(self, x):\n",
    "        output = torch.empty(0).to(self.device)\n",
    "        for layer in range(num_polytopes[0]): \n",
    "            output = torch.cat((output, self.module_list[layer].activation_pattern(x)))\n",
    "        return output\n",
    "    \n",
    "    def partition(self, w=1.6, title=\"img\"):\n",
    "        N = 200\n",
    "        x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "        grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        rank = (net(grid.view(-1,2))<0.001).float().view(N,N)                     # decision boundary, output<0\n",
    "        plt.contourf(x,y,rank.cpu(), np.arange(-1,2,1), cmap='gray')\n",
    "        plt.colorbar()\n",
    "        # dataset\n",
    "        plt.scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.')\n",
    "        plt.scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.')\n",
    "        plt.title(f\"Decision Boundary, Epoch: {self.epoch+1}\")\n",
    "        plt.savefig(folder_name+f\"/DB_{title}.png\")\n",
    "\n",
    "        for layer in range(num_polytopes[0]): \n",
    "            for i in range(self.module_list[layer].width):\n",
    "                plt.contour(x,y, self.module_list[layer].activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "        for layer in range(num_polytopes[0], num_polytopes[0]+num_polytopes[1]): \n",
    "            for i in range(self.module_list[layer].width):\n",
    "                plt.contour(x,y, self.module_list[layer].activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='red', linewidths=0.5)\n",
    "\n",
    "        black_patch = mpatches.Patch(color='black', label='DB')\n",
    "        blue_patch = mpatches.Patch(color='blue', label=r'$a_j=+1$')\n",
    "        red_patch = mpatches.Patch(color='red', label=r'$a_j=-1$')\n",
    "        plt.legend(handles=[blue_patch, red_patch])\n",
    "        plt.title(f\"AB and DB, Epoch: {self.epoch+1}\")\n",
    "        plt.savefig(folder_name+f\"/{title}.png\")\n",
    "#         plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    def pruning(self):\n",
    "        # for each polytope in the network, prun the useless widths.\n",
    "        for subnetwork_index, polytope in enumerate(self.module_list):\n",
    "            polytope.pruning(subnetwork_index=subnetwork_index)\n",
    "        self.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr) # net. paramgeter 를 optimizer 가능하도록 넣어줘야해...!!\n",
    "        \n",
    "    def rescaling_the_boundary_neurons(self, rescale=1.1):\n",
    "        # for each polytope in the network, prun the useless widths.\n",
    "        for subnetwork_index, polytope in enumerate(self.module_list):\n",
    "            polytope.rescaling_the_boundary_neurons(subnetwork_index=subnetwork_index, rescale=rescale)\n",
    "        self.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr) # net. paramgeter 를 optimizer 가능하도록 넣어줘야해...!!\n",
    "        \n",
    "    def convexity_check(self):\n",
    "        for subnetwork_index, polytope in enumerate(self.module_list):\n",
    "            assert polytope.convexity() == 0, f\"The subnetwork {subnetwork_index} is not convex\"\n",
    "            pass\n",
    "    \n",
    "    # Save result txt file.\n",
    "    def save_result_txt(self):\n",
    "        # Save result txt file.\n",
    "        f = open(f\"./{folder_name}/result.txt\", 'w')\n",
    "        f.write(\n",
    "        f\"\"\"\n",
    "            This is a CFG file.\n",
    "\n",
    "            # Accuracy\n",
    "            Trn_acc : {self.trn_acc_tr[-1] :.3f}\n",
    "            Test_acc : {self.test_acc_tr[-1] :.3f}\n",
    "\n",
    "            # Loss\n",
    "            Trn_loss : {self.trn_loss_tr[-1].item() :.4f} \n",
    "            Test_loss : {self.test_loss_tr[-1].item() :.4f} \n",
    "\n",
    "\n",
    "            # dataset\n",
    "            dataset = {dataset}\n",
    "            n = {n} # number of data\n",
    "            d = {d} # dimension\n",
    "            num_class = {num_classes} # the class, used in this binary classification\n",
    "            num_polytopes = f{num_polytopes} \n",
    "            width = f{width}\n",
    "            # Network\n",
    "            {net}\n",
    "\n",
    "            # optimization\n",
    "            Epochs = {Epochs}\n",
    "            lr = {lr}\n",
    "            optimizer = {optimizer}\n",
    "            Last epoch = {self.epoch+1}\n",
    "        \"\"\"\n",
    "        )\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    def save_loss_graph(self):\n",
    "        # plot and save the graphs\n",
    "        # PLOT THE LOSS GRAPH\n",
    "        plt.plot(self.trn_loss_tr, label=\"Train\")\n",
    "        plt.plot(self.test_loss_tr, label=\"Test\")\n",
    "        plt.xlabel(\"Iterations (k)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.legend()\n",
    "        plt.savefig(folder_name+\"/loss_fig_fullview.png\")\n",
    "        plt.show() if pt_option else None\n",
    "        plt.close()\n",
    "\n",
    "        # PLOT THE ACCURACY GRAPH\n",
    "        plt.plot(self.trn_acc_tr, label=\"Train\")\n",
    "        plt.plot(self.test_acc_tr, label=\"Test\")\n",
    "        plt.xlabel(\"Iterations (k)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.savefig(folder_name+\"/Accuracy.png\")\n",
    "        plt.show() if pt_option else None\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "    ### visualize every polytope...\n",
    "    def partition_polytopes(self, title=\"\"):\n",
    "        w=1.6\n",
    "        N = 200\n",
    "#         col = 3\n",
    "        x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "        grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "\n",
    "        # positive polytopes\n",
    "        fig, axs = plt.subplots(int(np.ceil(num_polytopes[0]/col)), col, figsize=(4*col, 4*int(np.ceil(num_polytopes[0]/col))), squeeze=False) # col figures in each row\n",
    "        for layer in range(num_polytopes[0]):\n",
    "            rank = F.relu(self.module_list[layer](grid).view(N,N).detach().cpu())\n",
    "            for i in range(self.module_list[layer].width):\n",
    "                axs[int(layer/col), layer%col].contour(x,y, self.module_list[layer].activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i],\n",
    "                                                   levels=1, colors='blue', linewidths=0.5)\n",
    "                axs[int(layer/col), layer%col].contourf(x,y,rank.cpu(), cmap='gray')\n",
    "                axs[int(layer/col), layer%col].set_title(fr\"Polytope $P_{layer+1}$, width={self.module_list[layer].width}\")\n",
    "                axs[int(layer/col), layer%col].scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.', color=\"C0\")\n",
    "                axs[int(layer/col), layer%col].scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.', color=\"C1\")\n",
    "            if self.module_list[layer].width == 0:\n",
    "                axs[int(layer/col), layer%col].set_title(fr\"Polytope $P_{layer+1}$, width={self.module_list[layer].width}\")\n",
    "                axs[int(layer/col), layer%col].scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.', color=\"C0\")\n",
    "                axs[int(layer/col), layer%col].scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.', color=\"C1\")\n",
    "                axs[int(layer/col), layer%col].set_xlim([-w, w])\n",
    "                axs[int(layer/col), layer%col].set_ylim([-w, w])\n",
    "        plt.suptitle(fr\"The shape of polytopes in the group $P$, epoch={title}\")\n",
    "        plt.savefig(folder_name+f\"/positive_AB_{title}.png\")\n",
    "    #     plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        ## negative polytopes\n",
    "        fig, axs = plt.subplots(int(np.ceil(num_polytopes[1]/col)), col, figsize=(4*col, 4*int(np.ceil(num_polytopes[1]/col))), squeeze=False) # col figures in each row\n",
    "        for layer in range(num_polytopes[1]): \n",
    "            rank = F.relu(self.module_list[num_polytopes[0]+layer](grid).view(N,N).detach().cpu())\n",
    "            for i in range(self.module_list[num_polytopes[0]+layer].width):\n",
    "                axs[int(layer/col), layer%col].contour(x,y, self.module_list[num_polytopes[0]+layer].activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i],\n",
    "                                                       levels=1, colors='red', linewidths=0.5)\n",
    "                axs[int(layer/col), layer%col].contourf(x,y,rank.cpu(), cmap='gray')\n",
    "                axs[int(layer/col), layer%col].set_title(fr\"Polytope $Q_{layer+1}$, width={self.module_list[num_polytopes[0]+layer].width}\")\n",
    "                axs[int(layer/col), layer%col].scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.', color=\"C0\")\n",
    "                axs[int(layer/col), layer%col].scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.', color=\"C1\")\n",
    "            if self.module_list[num_polytopes[0]+layer].width == 0:\n",
    "                axs[int(layer/col), layer%col].set_title(fr\"Polytope $Q_{layer+1}$, width={self.module_list[num_polytopes[0]+layer].width}\")\n",
    "                axs[int(layer/col), layer%col].scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.', color=\"C0\")\n",
    "                axs[int(layer/col), layer%col].scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.', color=\"C1\")\n",
    "                axs[int(layer/col), layer%col].set_xlim([-w, w])\n",
    "                axs[int(layer/col), layer%col].set_ylim([-w, w])\n",
    "        plt.suptitle(fr\"The shape of polytopes in the group $Q$, epoch={title}\")\n",
    "        plt.savefig(folder_name+f\"/negativie_AB_{title}.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # Training\n",
    "        self.trn_loss_tr = np.empty(0)\n",
    "        self.test_loss_tr = np.empty(0)\n",
    "        self.trn_acc_tr = np.empty(0)\n",
    "        self.test_acc_tr = np.empty(0)\n",
    "\n",
    "\n",
    "        test_acc = 0\n",
    "        test_loss = torch.zeros(1)\n",
    "        print(\"Start Training\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        log_period = 1000\n",
    "        for epoch in range(Epochs) :\n",
    "            self.epoch=epoch\n",
    "#             loss = criterion(self.forward(trn_X, partial_sum='positive'), trn_Y)\n",
    "#             index = (self.forward(trn_X, partial_sum='positive') == self.module_list[0].bias ).view(-1)\n",
    "#             loss += criterion(self.forward(trn_X[index], partial_sum='negative'), trn_Y[index])\n",
    "            \n",
    "            loss = criterion(self.forward(trn_X), trn_Y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            if epoch ==0 or epoch % pruning_period == (pruning_period-1) : # perturb every 50000 steps\n",
    "                self.trn_loss_tr = np.append(self.trn_loss_tr, loss.item())\n",
    "                self.test_loss_tr = np.append(self.test_loss_tr, test_loss.item())\n",
    "\n",
    "                trn_acc = ((net(trn_X)>0).float() == trn_Y).sum().item() / len(trn_Y) *100\n",
    "                self.trn_acc_tr = np.append(self.trn_acc_tr, trn_acc)\n",
    "                self.test_acc_tr = np.append(self.test_acc_tr, test_acc)\n",
    "                writer.add_scalars(\"ReLU/Loss\", {\n",
    "                    \"Trn_Loss\": loss.item(),\n",
    "                    \"Test_Loss\": test_loss.item(), }\n",
    "                                   , epoch+1)\n",
    "                writer.add_scalars(\"ReLU/Accuracy\", {\n",
    "                    \"Trn_acc\": trn_acc,\n",
    "                    \"Test_acc\": test_acc, }\n",
    "                                   , epoch+1)\n",
    "#                 if pt_option :\n",
    "                print(f\"Epoch: {epoch+1 :>5} || TRN_loss: {loss.item() :.4f} || TEST_loss: {test_loss.item():.4f} || TRN_ACC: {trn_acc:.3f} || TEST_ACC: {test_acc:.3f}\")\n",
    "                torch.save(net.state_dict(), folder_name+f\"/saved_net_width_{width}.pt\") \n",
    "                \n",
    "                if epoch > start_pruning : ### finetuning\n",
    "                    print(\"Strat Finetuning\")  if epoch==start_pruning else None\n",
    "#                     self.pruning() #### Pruning !!!!! ######\n",
    "                    self.rescaling_the_boundary_neurons(rescale=lambda_rescale) #### move neurons to boundary #######\n",
    "                self.partition(title=f\"{epoch+1}\") ### save partition images\n",
    "                self.partition_polytopes(title=f\"{epoch+1}\") ### save each polytopes separately\n",
    "                self.convexity_check() # raise error if not every polytope is convex\n",
    "                \n",
    "        self.save_loss_graph()\n",
    "        self.save_result_txt()\n",
    "        time.sleep(1)\n",
    "        print(f\"===================================   Training finished, Repetition: {repetition+1}  ================================== \\n\\n\") if pt_option else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe4cf5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch:     1 || TRN_loss: 0.6930 || TEST_loss: 0.0000 || TRN_ACC: 50.900 || TEST_ACC: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2000 || TRN_loss: 0.4982 || TEST_loss: 0.0000 || TRN_ACC: 84.800 || TEST_ACC: 0.000\n",
      "Epoch:  4000 || TRN_loss: 0.4578 || TEST_loss: 0.0000 || TRN_ACC: 93.300 || TEST_ACC: 0.000\n",
      "Epoch:  6000 || TRN_loss: 0.4466 || TEST_loss: 0.0000 || TRN_ACC: 97.800 || TEST_ACC: 0.000\n",
      "Epoch:  8000 || TRN_loss: 0.4430 || TEST_loss: 0.0000 || TRN_ACC: 99.300 || TEST_ACC: 0.000\n",
      "Epoch: 10000 || TRN_loss: 0.4417 || TEST_loss: 0.0000 || TRN_ACC: 99.800 || TEST_ACC: 0.000\n",
      "Epoch: 12000 || TRN_loss: 0.4412 || TEST_loss: 0.0000 || TRN_ACC: 99.900 || TEST_ACC: 0.000\n",
      "\t(rescaling)    In the subnetwork 0, 8 neurons are rescaled by 1.2\n",
      "\t(rescaling)    In the subnetwork 1, 8 neurons are rescaled by 1.2\n",
      "Epoch: 14000 || TRN_loss: 0.4406 || TEST_loss: 0.0000 || TRN_ACC: 100.000 || TEST_ACC: 0.000\n",
      "\t(rescaling)    In the subnetwork 1, 8 neurons are rescaled by 1.2\n",
      "Epoch: 16000 || TRN_loss: 0.4404 || TEST_loss: 0.0000 || TRN_ACC: 99.900 || TEST_ACC: 0.000\n",
      "\t(rescaling)    In the subnetwork 1, 8 neurons are rescaled by 1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x00000232668A5B80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m### training ######\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 219\u001b[0m, in \u001b[0;36mcover.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m pruning_period \u001b[38;5;241m==\u001b[39m (pruning_period\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) : \u001b[38;5;66;03m# perturb every 50000 steps\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrn_loss_tr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrn_loss_tr, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\decorators.py:50\u001b[0m, in \u001b[0;36mdisable\u001b[1;34m(fn, recursive)\u001b[0m\n\u001b[0;32m     48\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:406\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:961\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m linecache\u001b[38;5;241m.\u001b[39mcache:\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[0;32m    963\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optimization setting\n",
    "lr = 0.0001\n",
    "net = cover(width=width, num_polytopes=num_polytopes, positive_init=positive_init, small_norm_init=small_norm_init).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "folder_name = output_path + f'/runs/{dataset}/' + datetime.datetime.now().strftime(\"%B%d_%H_%M_%S\")\n",
    "writer = SummaryWriter(folder_name)\n",
    "plt.scatter(trn_X[:int(n/2),0].cpu(), trn_X[:int(n/2),1].cpu(), label='class 1')\n",
    "plt.scatter(trn_X[int(n/2):,0].cpu(), trn_X[int(n/2):,1].cpu(), label='class 0')\n",
    "plt.legend()\n",
    "plt.title(f\"{dataset} dataset\") \n",
    "plt.savefig(folder_name+\"/0_dataset.png\") \n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "### training ######\n",
    "net.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
