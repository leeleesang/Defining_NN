{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfc9872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classification, Intrinsic dimension, Degeneracy\n",
    "%run SetUp.ipynb\n",
    "%run synthetic_datasets.ipynb\n",
    "\n",
    "## Configuration \n",
    "Title = \"Rebuttal\"\n",
    "device = \"cuda:1\"\n",
    "pt_option=True\n",
    "width = 8\n",
    "dataset = \"Swiss\" # Swiss / Two circles / Two moons / XOR\n",
    "d = 2\n",
    "n = 1000 # used number of data\n",
    "num_classes = 2\n",
    "selected_class = 0\n",
    "data_balance = 1\n",
    "trn_X, trn_Y = load_synthetic_dataset(dataset)\n",
    "\n",
    "# create directory\n",
    "output_path = f\"./Results/{Title}\"\n",
    "create_directory(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b702e",
   "metadata": {},
   "source": [
    "Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec239261",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Network setting # d -> d1 -> 1\n",
    "class polytope(nn.Module):\n",
    "    def __init__(self, width, output_class=1, positive_init=False, small_norm_init=False):\n",
    "        super(polytope, self).__init__()\n",
    "        self.fc0 = nn.Linear(d, width)\n",
    "        self.fc1 = nn.Linear(width, output_class, bias=False)\n",
    "        self.width = width # width\n",
    "        self.epoch = 0\n",
    "        self.data_balance = data_balance\n",
    "        self.bias = (1 - 2*positive_init)*3\n",
    "        self.positive_init = positive_init\n",
    "        self.output_class = output_class\n",
    "        self.small_norm_init = small_norm_init\n",
    "        self.device = device\n",
    "        \n",
    "        # initialization\n",
    "        if small_norm_init:\n",
    "            self.change_layer_weights(layer=0, W=1.5/self.width*torch.randn_like(self.W(0)), b=0.6/self.width*(self.b(0))) # multiply 0.01 \n",
    "            \n",
    "        if positive_init:\n",
    "            # if positivie init,  all v_k are positive.\n",
    "            self.fc1.weight = nn.Parameter(torch.sqrt((self.W(0).norm(dim=1)**2 + self.b(0)**2).view(1,-1)+1))\n",
    "        else:\n",
    "            # else, to all v_k are negative.\n",
    "            self.fc1.weight = nn.Parameter(-torch.sqrt((self.W(0).norm(dim=1)**2 + self.b(0)**2).view(1,-1)+1))\n",
    "        \n",
    "        # declare the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.g1 = self.fc0(x)\n",
    "        self.h1 = F.relu(self.g1)\n",
    "        self.g2 = self.fc1(self.h1) + self.bias\n",
    "        return self.g2\n",
    "    \n",
    "    def W(self, layer):\n",
    "        if layer ==0:\n",
    "            output = self.fc0.weight\n",
    "        elif layer ==1:\n",
    "            output = self.fc1.weight\n",
    "        return output.clone()\n",
    "    def b(self, layer):\n",
    "        if layer ==0:\n",
    "            output = self.fc0.bias\n",
    "        elif layer ==1:\n",
    "            output = self.fc1.bias\n",
    "        return output if output == None else output.clone()\n",
    "    \n",
    "    def activation_pattern(self, x):\n",
    "        self.forward(x)\n",
    "        return (self.h1>0).float()\n",
    "    \n",
    "    def change_layer_weights(self, layer, W, b):\n",
    "        if self.b(layer) == None : # no bias term\n",
    "            if W.shape == self.W(layer).shape and self.b(layer) == b:\n",
    "                if layer ==0:\n",
    "                    self.fc0.weight = nn.Parameter(W.to(self.device))\n",
    "                elif layer ==1:\n",
    "                    self.fc1.weight = nn.Parameter(W.to(self.device))\n",
    "            else:\n",
    "                raise ValueError(\"wrong shape of input tensors\")\n",
    "        else:\n",
    "            if W.shape == self.W(layer).shape and b.shape == self.b(layer).shape:\n",
    "                if layer ==0:\n",
    "                    self.fc0.weight = nn.Parameter(W.to(self.device))\n",
    "                    self.fc0.bias = nn.Parameter(b.to(self.device))\n",
    "                elif layer ==1:\n",
    "                    self.fc1.weight = nn.Parameter(W.to(self.device))\n",
    "    #                 self.fc1.bias = nn.Parameter(b)\n",
    "            else:\n",
    "                raise ValueError(\"wrong shape of input tensors\")\n",
    "    \n",
    "    def partition(self, save_location, trn_X_index, w=1.6, title=\"title\"):\n",
    "        N = 200\n",
    "        x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "        grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "        rank = (self.forward(grid.view(-1,2))<0.001).float().view(N,N)                     # decision boundary, output<0\n",
    "        plt.contourf(x,y,rank.cpu(), np.arange(-1,2,1), cmap='gray')\n",
    "        plt.contourf(x,y,(self.forward(grid.view(-1,2))==self.bias).float().cpu().view(N,N), np.arange(-1,2,1), cmap='Greys')\n",
    "        plt.colorbar()\n",
    "        for i in range(self.width):\n",
    "            plt.contour(x,y, self.activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "\n",
    "        # dataset\n",
    "        plt.scatter(trn_X[:int(n/2)][trn_X_index[:int(n/2)],0].cpu(),  trn_X[:int(n/2)][trn_X_index[:int(n/2)],1].cpu(), marker='.', color=\"C0\")\n",
    "        plt.scatter(trn_X[int(n/2):][trn_X_index[int(n/2):],0].cpu(),  trn_X[int(n/2):][trn_X_index[int(n/2):],1].cpu(), marker='.', color=\"C1\")\n",
    "        plt.title(title)\n",
    "        plt.savefig(save_location)\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    def train(self, repetition, trn_X_index):\n",
    "        self.fail = False\n",
    "        self.trn_loss_tr = np.empty(0)\n",
    "        self.test_loss_tr = np.empty(0)\n",
    "        self.trn_acc_tr = np.empty(0)\n",
    "        self.test_acc_tr = np.empty(0)\n",
    "\n",
    "\n",
    "        test_acc = 0\n",
    "        test_loss = torch.zeros(1)\n",
    "        print(f\"=================================   Start Training. Repetition: {repetition+1}   ==================================\") if pt_option else None\n",
    "        time.sleep(1)\n",
    "\n",
    "        log_period = 1000\n",
    "        for epoch in range(Epochs) :\n",
    "            self.epoch = epoch\n",
    "#             loss = criterion(self.forward(trn_X[trn_X_index]), trn_Y[trn_X_index])\n",
    "            # since class may be imbalance\n",
    "            loss =  criterion(net(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(positive_init)).view(-1)]),\n",
    "                              trn_Y[trn_X_index][(trn_Y[trn_X_index]==float(positive_init)).view(-1)]) / len((trn_Y[trn_X_index]==float(positive_init)).view(-1))\n",
    "            loss += criterion(net(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(not positive_init)).view(-1)]), \n",
    "                              trn_Y[trn_X_index][(trn_Y[trn_X_index]==float(not positive_init)).view(-1)]) / len((trn_Y[trn_X_index]==float(not positive_init)).view(-1)) * self.data_balance\n",
    "            loss *= len((trn_Y[trn_X_index]).view(-1))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            if epoch ==0 or epoch % 1000 == 999 :\n",
    "                self.trn_loss_tr = np.append(self.trn_loss_tr, loss.item())\n",
    "                self.test_loss_tr = np.append(self.test_loss_tr, test_loss.item())\n",
    "\n",
    "                trn_acc = ((net(trn_X)>-0.001).float() == trn_Y).sum().item() / len(trn_Y) *100\n",
    "                self.trn_acc_tr = np.append(self.trn_acc_tr, trn_acc)\n",
    "                self.test_acc_tr = np.append(self.test_acc_tr, test_acc)\n",
    "                class0_inPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]!=float(self.positive_init)).view(-1)]) == self.bias).sum().item()\n",
    "                class1_inPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(self.positive_init)).view(-1)]) == self.bias).sum().item()\n",
    "                class0_outPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]!=float(self.positive_init)).view(-1)]) != self.bias).sum().item()\n",
    "                class1_outPoly = (self.forward(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(self.positive_init)).view(-1)]) != self.bias).sum().item()\n",
    "#                 self.data_balance = min(100, max(50, class1_outPoly/(.1 + class0_outPoly)))\n",
    "                self.data_balance = 100\n",
    "                writer.add_scalars(\"ReLU/Loss\", {\n",
    "                    \"Trn_Loss\": loss.item(),\n",
    "                    \"Test_Loss\": test_loss.item(), }\n",
    "                                   , epoch+1)\n",
    "                writer.add_scalars(\"ReLU/Accuracy\", {\n",
    "                    \"Trn_acc\": trn_acc,\n",
    "                    \"Test_acc\": test_acc, }\n",
    "                                   , epoch+1)\n",
    "                writer.add_scalars(\"ReLU/InOutClass\", {\n",
    "                    \"class0_inPoly\": class0_inPoly,\n",
    "                    \"class0_outPoly\": class0_outPoly,\n",
    "                    \"class0_inPoly\": class1_inPoly,\n",
    "                    \"class0_outPoly\": class1_outPoly,}\n",
    "                                   , epoch+1)\n",
    "                if pt_option :\n",
    "                    print(f\"Epoch: {epoch+1 :>5} || TRN_loss: {loss.item() :.4f} || TRN_ACC: {trn_acc:.3f}\", end=\"\")\n",
    "                    print(f\" || balanced_min: {self.fc1.weight.abs().min():.3f} || class0_inPoly : {class0_inPoly}\", end=\"\")\n",
    "                    print(f\" || class0_outPoly : {class0_outPoly} || class1_inPoly : {class1_inPoly} || class1_outPoly : {class1_outPoly}\")\n",
    "#                     print(net.fc0.weight.norm(dim=1)**2 + net.fc0.bias**2 - net.fc1.weight.norm(dim=0)**2) # balanced\n",
    "                torch.save(self.state_dict(), folder_name+f\"/Rep{repetition+1}_saved_net_width_{width}.pt\") \n",
    "                # Pruning and merging\n",
    "                self.pruning(trn_X[trn_X_index]) if epoch > 5555 else None ## pruning after pre-training\n",
    "                self.partition(trn_X_index=trn_X_index, title=f\"AB and DB, width={self.width}, epoch={self.epoch+1}\", save_location=folder_name+f\"/Rep{repetition+1}_{epoch+1}.png\")\n",
    "                \n",
    "                \n",
    "                if self.fc1.weight.sign().sum().abs() !=  self.width: # there is a flipped sign.\n",
    "                    print(\"Sign flipped !!\") if pt_option else None\n",
    "                    self.fail = True # skip\n",
    "                    break\n",
    "                \n",
    "                if self.width == 0 : # there is no active neuron.\n",
    "                    print(\"All neruons have been removed !!\") if pt_option else None\n",
    "                    self.fail = True # skip \n",
    "                    break\n",
    "\n",
    "                # terminating condition - if one class is completely surrounded by a convex polytope.\n",
    "                if (class0_outPoly == 0) and (epoch>39990):\n",
    "                    print(\"Found a complete convex polytope cover, escape the training loop\") if pt_option else None\n",
    "                    self.fail = False\n",
    "                    break\n",
    "        self.save_loss_graph()\n",
    "        self.save_result_txt()\n",
    "        self.partition(trn_X_index=trn_X_index, title=f\"Polytope cover {repetition+1}, width={self.width}\", save_location=folder_name_above+f\"/Cover_{repetition+1}.png\")\n",
    "        time.sleep(1)\n",
    "        print(f\"===================================   Training finished, Repetition: {repetition+1}  ================================== \\n\\n\") if pt_option else None\n",
    "        \n",
    "    \n",
    "    def save_loss_graph(self):\n",
    "        # plot and save the graphs\n",
    "        # PLOT THE LOSS GRAPH\n",
    "        plt.plot(self.trn_loss_tr, label=\"Train\")\n",
    "        plt.plot(self.test_loss_tr, label=\"Test\")\n",
    "        plt.xlabel(\"Iterations (k)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Rep{repetition+1}_Loss\")\n",
    "        plt.savefig(folder_name+\"/loss_fig_fullview.png\")\n",
    "#         plt.show() if pt_option else None\n",
    "        plt.close()\n",
    "\n",
    "        # PLOT THE ACCURACY GRAPH\n",
    "        plt.plot(self.trn_acc_tr, label=\"Train\")\n",
    "        plt.plot(self.test_acc_tr, label=\"Test\")\n",
    "        plt.xlabel(\"Iterations (k)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Rep{repetition+1}_Accuracy\")\n",
    "        plt.savefig(folder_name+\"/Accuracy.png\")\n",
    "#         plt.show() if pt_option else None\n",
    "        plt.close()\n",
    "    \n",
    "    # Save result txt file.\n",
    "    def save_result_txt(self):\n",
    "        class0_inPoly = (net(trn_X[trn_X_index][(trn_Y[trn_X_index]!=float(self.positive_init)).view(-1)]) == self.bias).sum().item()\n",
    "        class1_inPoly = (net(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(self.positive_init)).view(-1)]) == self.bias).sum().item()\n",
    "        class0_outPoly = (net(trn_X[trn_X_index][(trn_Y[trn_X_index]!=float(self.positive_init)).view(-1)]) != self.bias).sum().item()\n",
    "        class1_outPoly = (net(trn_X[trn_X_index][(trn_Y[trn_X_index]==float(self.positive_init)).view(-1)]) != self.bias).sum().item()\n",
    "        \n",
    "        f = open(f\"./{folder_name}/Rep{repetition+1}_result.txt\", 'w')\n",
    "        f.write(\n",
    "        f\"\"\"\n",
    "            This is a CFG file.\n",
    "\n",
    "            # Accuracy\n",
    "            Trn_acc : {self.trn_acc_tr[-1] :.3f}\n",
    "            Test_acc : {self.test_acc_tr[-1] :.3f}\n",
    "\n",
    "            # Loss\n",
    "            Trn_loss : {self.trn_loss_tr[-1].item() :.4f} \n",
    "            Test_loss : {self.test_loss_tr[-1].item() :.4f} \n",
    "\n",
    "\n",
    "            # dataset\n",
    "            dataset = {dataset}\n",
    "            n = {n} # number of total data\n",
    "            d = {d} # dimension\n",
    "            num_class = {num_classes} # the class, used in this binary classification\n",
    "            starting_width = {width}\n",
    "            final_width = {net.width}\n",
    "            # Network\n",
    "            {net}\n",
    "            \n",
    "            # Polytope\n",
    "            number of data used in this training (Class 0 / 1): {class0_inPoly+class0_outPoly} / {class1_inPoly+class1_outPoly}\n",
    "            Class 0 in Polytope : {class0_inPoly}\n",
    "            Class 1 in Polytope : {class1_inPoly}\n",
    "            Class 0 out Polytope : {class0_outPoly}\n",
    "            Class 1 out Polytope : {class1_outPoly}\n",
    "            \n",
    "            # optimization\n",
    "            Epochs = {Epochs}\n",
    "            lr = {lr}\n",
    "            optimizer = {self.optimizer}\n",
    "            Last epoch = {self.epoch+1}\n",
    "            \n",
    "            ## trn_X_index\n",
    "            {trn_X_index}\n",
    "        \"\"\"\n",
    "        )\n",
    "        f.close()\n",
    "    \n",
    "    def pruning(self, dataset):\n",
    "        width_before = self.width\n",
    "        dataset_activation_pattern = self.activation_pattern(dataset).sum(dim=0)\n",
    "        if dataset_activation_pattern.max() == len(dataset) or dataset_activation_pattern.min() == 0 : \n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            activation_filter = ((self.activation_pattern(dataset).sum(dim=0) < len(dataset)).float()\n",
    "                                 * (self.activation_pattern(dataset).sum(dim=0) > 0).float()\n",
    "                                 * (self.W(layer=1).abs()>0.1).float().view(-1)\n",
    "                                )\n",
    "            # change polytope width \n",
    "            self.width = activation_filter.sum().long().item()\n",
    "            width_after = self.width\n",
    "            self.fc0 = nn.Linear(d, self.width)\n",
    "            self.fc1 = nn.Linear(self.width, self.output_class, bias=False)\n",
    "            self.change_layer_weights(layer=0, W=W0[(activation_filter == 1)], b=b0[activation_filter == 1])\n",
    "            self.change_layer_weights(layer=1, W=W1.view(-1)[activation_filter == 1].view(1,-1), b=None)\n",
    "            text = \"(removing)\"\n",
    "            print(f\"\\t{text:<15}The network is prunned, width : {width_before} --> {width_after}\") if pt_option else None\n",
    "            \n",
    "        ## merging vectors with same activation patterns ##\n",
    "        width_activation_pattern = self.activation_pattern(dataset) # len(dataset) x width\n",
    "        width_activation_pattern_unique = width_activation_pattern.unique(dim=1).t()\n",
    "        width_before = self.width\n",
    "        if len(width_activation_pattern_unique) < self.width :\n",
    "            self.width = len(width_activation_pattern_unique) \n",
    "            width_after = self.width\n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            self.fc0 = nn.Linear(d, self.width)\n",
    "            self.fc1 = nn.Linear(self.width, self.output_class, bias=False)\n",
    "            self.to(device)\n",
    "            # build weight matrices\n",
    "            new_W0 = self.W(layer=0).detach()\n",
    "            new_b0 = self.b(layer=0).detach()\n",
    "            new_W1 = self.W(layer=1).detach()\n",
    "            for index, pattern in enumerate(width_activation_pattern_unique):\n",
    "                new_W0[index] = W0[(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "                new_b0[index] = b0[(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "                new_W1[0][index] = W1[0][(width_activation_pattern.t() == pattern).prod(dim=1)==1].sum(dim=0)\n",
    "            self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "            self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "            text = \"(merging)\"\n",
    "            print(f\"\\t{text:<15}The network is merged, width : {width_before} --> {width_after}\") if pt_option else None\n",
    "\n",
    "        ###### original #######\n",
    "        smallest_norm_neuron_index = self.width # not index yet.\n",
    "        smallest_norm = np.inf\n",
    "        width_activation_pattern = self.activation_pattern(dataset) # len(dataset) x width\n",
    "        for neuron_index in range(self.width): # for loop\n",
    "            activated_index = (width_activation_pattern[:, neuron_index] == 1)\n",
    "            # index 가 activated 된 data들의 activation pattern ## 이게 2 이상이면 제거해버리면 됨 !\n",
    "            if self.activation_pattern(dataset[activated_index]).sum(dim=1).min().item() >=2 :\n",
    "                # this neuron can be removed.\n",
    "                this_round_norm = (self.W(layer=0)[neuron_index].norm() * self.W(layer=1).squeeze()[neuron_index]).item() ## v||w||\n",
    "#                 this_round_norm = self.W(layer=1).squeeze()[neuron_index].item() ## v\n",
    "                if this_round_norm < smallest_norm:\n",
    "                    smallest_norm_neuron_index = neuron_index\n",
    "                    smallest_norm = this_round_norm\n",
    "        if smallest_norm_neuron_index < self.width:\n",
    "            # remove the small-norm redundant neuron, remove only one neuron at once\n",
    "            width_before = self.width\n",
    "            width_after = self.width - 1\n",
    "            W0 = self.W(layer=0).detach()\n",
    "            b0 = self.b(layer=0).detach()\n",
    "            W1 = self.W(layer=1).detach()\n",
    "            self.fc0 = nn.Linear(d, width_after)\n",
    "            self.fc1 = nn.Linear(width_after, self.output_class, bias=False)\n",
    "            self.to(device)        \n",
    "            new_W0 = torch.cat((W0[:smallest_norm_neuron_index], W0[smallest_norm_neuron_index+1:]), dim=0)\n",
    "            new_b0 = torch.cat((b0[:smallest_norm_neuron_index], b0[smallest_norm_neuron_index+1:]), dim=0)\n",
    "            new_W1 = torch.cat((W1[:,:smallest_norm_neuron_index], W1[:,smallest_norm_neuron_index+1:]), dim=1)\n",
    "            self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "            self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "            self.width = width_after\n",
    "            text = \"(pruning)\"\n",
    "            print(f\"\\t{text:<15}In the polytope, a redundant neuron has been removed\") if pt_option else None\n",
    "        \n",
    "        \n",
    "        ### rescaling function ## here, we don't need to rescale the neuron.\n",
    "        rescale=1.1\n",
    "        intermediate_index = ( ( F.relu(self.forward(dataset)) != 0).float() * (self.forward(dataset) != self.bias).float() >0).squeeze()\n",
    "        if intermediate_index.float().sum() >0 :\n",
    "            activation_pattern_of_intermediate_data = self.activation_pattern(dataset[intermediate_index])\n",
    "            if activation_pattern_of_intermediate_data.max().item() >0 : ## max\n",
    "                new_W0 = self.W(layer=0).detach()\n",
    "                new_b0 = self.b(layer=0).detach()\n",
    "                new_W1 = self.W(layer=1).detach()\n",
    "                true_or_false = activation_pattern_of_intermediate_data.sum(dim=0)>0\n",
    "                for i, boolean in enumerate(true_or_false):\n",
    "                    if boolean:\n",
    "                        new_W0[i] = new_W0[i] * rescale\n",
    "                        new_b0[i] = new_b0[i] * rescale\n",
    "                        new_W1[0][i] = new_W1[0][i] * rescale\n",
    "                self.change_layer_weights(layer=0, W=new_W0, b=new_b0)\n",
    "                self.change_layer_weights(layer=1, W=new_W1, b=None)\n",
    "                text = \"(rescaling)\"\n",
    "                print(f\"\\t{text:<15}In the polytope, {true_or_false.sum().item()} neurons are rescaled by {rescale}\") if pt_option else None\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr) # net. paramgeter 를 optimizer 가능하도록 넣어줘야해...!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb72e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimization setting\n",
    "Epochs = 150*1000\n",
    "lr = 0.0001\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# finding the polytope cover !\n",
    "repetition = 0\n",
    "trn_X_index_all = (torch.ones(len(trn_X))>0).to(device) # Boolean tensor, shape = len(dataset)\n",
    "trn_X_index = trn_X_index_all.clone()\n",
    "# trn_X_index *= (torch.randn(len(trn_X))>0.9).to(device) # random\n",
    "network_list = []\n",
    "folder_name_above = output_path + f'/runs/{dataset}/' + datetime.datetime.now().strftime(\"%B%d_%H_%M_%S\")\n",
    "create_directory(folder_name_above)\n",
    "# save the initial figure\n",
    "plt.scatter(trn_X[:int(n/2)][trn_X_index[:int(n/2)],0].cpu(),  trn_X[:int(n/2)][trn_X_index[:int(n/2)],1].cpu(), marker='.', color=\"C0\", label=\"class 0\")\n",
    "plt.scatter(trn_X[int(n/2):][trn_X_index[int(n/2):],0].cpu(),  trn_X[int(n/2):][trn_X_index[int(n/2):],1].cpu(), marker='.', color=\"C1\", label=\"class 1\")\n",
    "plt.title(\"Dataset\")\n",
    "plt.legend()\n",
    "plt.savefig(folder_name_above+\"/Dataset.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "993ed97c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Cover 1: What width do you want ? 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================   Start Training. Repetition: 1   ==================================\n",
      "Epoch:     1 || TRN_loss: 1.5500 || TRN_ACC: 41.900 || balanced_min: 1.024 || class0_inPoly : 0 || class0_outPoly : 500 || class1_inPoly : 0 || class1_outPoly : 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1000 || TRN_loss: 46.4491 || TRN_ACC: 61.500 || balanced_min: 0.984 || class0_inPoly : 0 || class0_outPoly : 500 || class1_inPoly : 0 || class1_outPoly : 500\n",
      "Epoch:  2000 || TRN_loss: 27.5455 || TRN_ACC: 50.000 || balanced_min: 0.930 || class0_inPoly : 0 || class0_outPoly : 500 || class1_inPoly : 0 || class1_outPoly : 500\n",
      "Epoch:  3000 || TRN_loss: 19.2140 || TRN_ACC: 50.000 || balanced_min: 0.896 || class0_inPoly : 0 || class0_outPoly : 500 || class1_inPoly : 0 || class1_outPoly : 500\n",
      "Epoch:  4000 || TRN_loss: 14.8389 || TRN_ACC: 50.000 || balanced_min: 0.879 || class0_inPoly : 0 || class0_outPoly : 500 || class1_inPoly : 0 || class1_outPoly : 500\n",
      "Epoch:  5000 || TRN_loss: 12.3245 || TRN_ACC: 50.000 || balanced_min: 0.876 || class0_inPoly : 0 || class0_outPoly : 500 || class1_inPoly : 0 || class1_outPoly : 500\n",
      "Epoch:  6000 || TRN_loss: 10.6392 || TRN_ACC: 50.000 || balanced_min: 0.836 || class0_inPoly : 0 || class0_outPoly : 500 || class1_inPoly : 0 || class1_outPoly : 500\n",
      "\t(removing)     The network is prunned, width : 15 --> 11\n",
      "\t(pruning)      In the polytope, a redundant neuron has been removed\n",
      "Epoch:  7000 || TRN_loss: 8.5499 || TRN_ACC: 50.000 || balanced_min: 0.845 || class0_inPoly : 0 || class0_outPoly : 500 || class1_inPoly : 0 || class1_outPoly : 500\n",
      "\t(removing)     The network is prunned, width : 10 --> 8\n",
      "\t(pruning)      In the polytope, a redundant neuron has been removed\n",
      "Epoch:  8000 || TRN_loss: 7.8570 || TRN_ACC: 50.000 || balanced_min: 0.794 || class0_inPoly : 164 || class0_outPoly : 336 || class1_inPoly : 97 || class1_outPoly : 403\n",
      "\t(pruning)      In the polytope, a redundant neuron has been removed\n",
      "Epoch:  9000 || TRN_loss: 7.6874 || TRN_ACC: 50.000 || balanced_min: 0.833 || class0_inPoly : 446 || class0_outPoly : 54 || class1_inPoly : 324 || class1_outPoly : 176\n",
      "\t(pruning)      In the polytope, a redundant neuron has been removed\n",
      "Epoch: 10000 || TRN_loss: 7.6892 || TRN_ACC: 50.000 || balanced_min: 0.981 || class0_inPoly : 453 || class0_outPoly : 47 || class1_inPoly : 324 || class1_outPoly : 176\n",
      "\t(merging)      The network is merged, width : 5 --> 4\n",
      "\t(pruning)      In the polytope, a redundant neuron has been removed\n",
      "Epoch: 11000 || TRN_loss: 7.6590 || TRN_ACC: 50.000 || balanced_min: 1.590 || class0_inPoly : 455 || class0_outPoly : 45 || class1_inPoly : 324 || class1_outPoly : 176\n",
      "Epoch: 12000 || TRN_loss: 7.6282 || TRN_ACC: 50.000 || balanced_min: 1.692 || class0_inPoly : 457 || class0_outPoly : 43 || class1_inPoly : 325 || class1_outPoly : 175\n",
      "Epoch: 13000 || TRN_loss: 7.5977 || TRN_ACC: 50.000 || balanced_min: 1.794 || class0_inPoly : 458 || class0_outPoly : 42 || class1_inPoly : 326 || class1_outPoly : 174\n",
      "Epoch: 14000 || TRN_loss: 7.5658 || TRN_ACC: 50.100 || balanced_min: 1.895 || class0_inPoly : 461 || class0_outPoly : 39 || class1_inPoly : 330 || class1_outPoly : 170\n",
      "\t(rescaling)    In the polytope, 2 neurons are rescaled by 1.1\n",
      "Epoch: 15000 || TRN_loss: 7.4997 || TRN_ACC: 52.600 || balanced_min: 2.167 || class0_inPoly : 466 || class0_outPoly : 34 || class1_inPoly : 330 || class1_outPoly : 170\n",
      "\t(rescaling)    In the polytope, 2 neurons are rescaled by 1.1\n",
      "Epoch: 16000 || TRN_loss: 7.4380 || TRN_ACC: 54.800 || balanced_min: 2.268 || class0_inPoly : 468 || class0_outPoly : 32 || class1_inPoly : 330 || class1_outPoly : 170\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 17000 || TRN_loss: 7.3609 || TRN_ACC: 55.500 || balanced_min: 2.596 || class0_inPoly : 472 || class0_outPoly : 28 || class1_inPoly : 328 || class1_outPoly : 172\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 18000 || TRN_loss: 7.2831 || TRN_ACC: 56.800 || balanced_min: 2.955 || class0_inPoly : 477 || class0_outPoly : 23 || class1_inPoly : 327 || class1_outPoly : 173\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 19000 || TRN_loss: 7.2039 || TRN_ACC: 57.800 || balanced_min: 3.351 || class0_inPoly : 480 || class0_outPoly : 20 || class1_inPoly : 328 || class1_outPoly : 172\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 20000 || TRN_loss: 7.1279 || TRN_ACC: 60.700 || balanced_min: 3.785 || class0_inPoly : 485 || class0_outPoly : 15 || class1_inPoly : 327 || class1_outPoly : 173\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 21000 || TRN_loss: 7.0609 || TRN_ACC: 62.700 || balanced_min: 4.264 || class0_inPoly : 489 || class0_outPoly : 11 || class1_inPoly : 326 || class1_outPoly : 174\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 22000 || TRN_loss: 7.0065 || TRN_ACC: 64.200 || balanced_min: 4.790 || class0_inPoly : 491 || class0_outPoly : 9 || class1_inPoly : 326 || class1_outPoly : 174\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 23000 || TRN_loss: 6.9660 || TRN_ACC: 64.700 || balanced_min: 5.371 || class0_inPoly : 495 || class0_outPoly : 5 || class1_inPoly : 328 || class1_outPoly : 172\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 24000 || TRN_loss: 6.9371 || TRN_ACC: 65.500 || balanced_min: 6.007 || class0_inPoly : 495 || class0_outPoly : 5 || class1_inPoly : 328 || class1_outPoly : 172\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 25000 || TRN_loss: 6.9165 || TRN_ACC: 66.100 || balanced_min: 6.713 || class0_inPoly : 497 || class0_outPoly : 3 || class1_inPoly : 328 || class1_outPoly : 172\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 26000 || TRN_loss: 6.9022 || TRN_ACC: 66.300 || balanced_min: 7.487 || class0_inPoly : 496 || class0_outPoly : 4 || class1_inPoly : 328 || class1_outPoly : 172\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 27000 || TRN_loss: 6.8915 || TRN_ACC: 66.400 || balanced_min: 8.337 || class0_inPoly : 498 || class0_outPoly : 2 || class1_inPoly : 329 || class1_outPoly : 171\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 28000 || TRN_loss: 6.8842 || TRN_ACC: 66.700 || balanced_min: 9.271 || class0_inPoly : 500 || class0_outPoly : 0 || class1_inPoly : 329 || class1_outPoly : 171\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 29000 || TRN_loss: 6.8792 || TRN_ACC: 66.800 || balanced_min: 10.297 || class0_inPoly : 500 || class0_outPoly : 0 || class1_inPoly : 329 || class1_outPoly : 171\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 30000 || TRN_loss: 6.8758 || TRN_ACC: 66.800 || balanced_min: 11.426 || class0_inPoly : 498 || class0_outPoly : 2 || class1_inPoly : 329 || class1_outPoly : 171\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 31000 || TRN_loss: 6.8732 || TRN_ACC: 66.900 || balanced_min: 12.669 || class0_inPoly : 500 || class0_outPoly : 0 || class1_inPoly : 329 || class1_outPoly : 171\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 32000 || TRN_loss: 6.8713 || TRN_ACC: 66.900 || balanced_min: 14.035 || class0_inPoly : 498 || class0_outPoly : 2 || class1_inPoly : 329 || class1_outPoly : 171\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n",
      "Epoch: 33000 || TRN_loss: 6.8696 || TRN_ACC: 66.900 || balanced_min: 15.539 || class0_inPoly : 500 || class0_outPoly : 0 || class1_inPoly : 329 || class1_outPoly : 171\n",
      "\t(rescaling)    In the polytope, 3 neurons are rescaled by 1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_init = True ### if positive_init=True ==> polytope is negative (-1) ==> approximating the orange class (label 0).\n",
    "# blue class is labeld by one(+1)\n",
    "while trn_X_index[(trn_Y==0).view(-1)].sum()>0 and trn_X_index[(trn_Y==1).view(-1)].sum()>0 :\n",
    "    width = int(input(f\"Cover {repetition+1}: What width do you want ?\")) ## manually adjust the width\n",
    "    net = polytope(width=width, positive_init=positive_init, small_norm_init=False).to(device)\n",
    "    folder_name = folder_name_above + f\"/Cover_{repetition+1}\"\n",
    "    writer = SummaryWriter(folder_name)\n",
    "    \n",
    "    # training\n",
    "    trn_X_index_this_time = trn_X_index.clone()\n",
    "    ####\n",
    "    trn_X_index_this_time[(trn_Y==float(positive_init)).view(-1)] = True  #### consider all data points in the other class\n",
    "    ####\n",
    "    net.train(repetition=repetition, trn_X_index= trn_X_index_this_time)\n",
    "    ans = input(\"Wanna train again ? (y/n)\")\n",
    "    if ans == \"y\":\n",
    "        net.fail = True\n",
    "    # after trainnig, extract the index.\n",
    "    trn_X_index_in_polytope = ((net(trn_X).view(-1) * trn_X_index)  == net.bias) # boolean tensor, shape = len(dataset)\n",
    "    # new index\n",
    "    trn_X_index = trn_X_index * trn_X_index_in_polytope\n",
    "    if net.fail:\n",
    "        # there is some problem for finding a cover in this training time.\n",
    "        # therefore, we skip this try and re-train.\n",
    "        positive_init = not positive_init # flip the convexity of the class.\n",
    "        continue\n",
    "    # otherwise, it was successful.\n",
    "    network_list.append(net)\n",
    "    positive_init = not positive_init # flip the convexity of the class.\n",
    "    repetition += 1\n",
    "\n",
    "# result\n",
    "if pt_option :\n",
    "    print(\"\\n\\nCompleted to find a polytope-basis cover!!\")\n",
    "    print(f\"There are {len(network_list)} polytopes.\")\n",
    "    print(network_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590df01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### visualize every polytope...\n",
    "w = 1.6\n",
    "N = 200\n",
    "x,y = torch.meshgrid(torch.linspace(-w,w,N), torch.linspace(-w,w,N))\n",
    "grid = torch.stack((x,y),dim=2).to(device).float() # SHAPE 10,10,2\n",
    "\n",
    "fig, axs = plt.subplots(int(np.ceil(len(network_list)/4)), 4, figsize=(16, 4*int(np.ceil(len(network_list)/4))), squeeze=False) # 4 figures in each row\n",
    "\n",
    "for layer, net in enumerate(network_list):\n",
    "    rank = (net(grid).view(N,N).detach().cpu() == net.bias).float()\n",
    "#     for i in range(net.width):\n",
    "#         axs[int(layer/4), layer%4].contour(x,y, net.activation_pattern(grid.view(-1,2)).float().cpu().view(N,N,-1)[:,:,i], levels=1, colors='blue', linewidths=0.5)\n",
    "    axs[int(layer/4), layer%4].contourf(x,y,rank.cpu(), cmap='Greys') ###\n",
    "    axs[int(layer/4), layer%4].contour(x,y,rank.cpu(), cmap='Greens', linewidths=2) ###\n",
    "    axs[int(layer/4), layer%4].set_title(f\"Polytope {layer+1}, width={net.width}\")\n",
    "    axs[int(layer/4), layer%4].scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.', color=\"C0\")\n",
    "    axs[int(layer/4), layer%4].scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.', color=\"C1\")\n",
    "plt.suptitle(r\"The polytope-basis cover\")\n",
    "plt.savefig(folder_name_above+\"/polytopes.png\")\n",
    "plt.close()\n",
    "\n",
    "########### Accuracy #########\n",
    "answer = torch.zeros_like(net(trn_X))\n",
    "previous_answer = torch.ones_like(net(trn_X))\n",
    "for exponential, net in enumerate(network_list):\n",
    "    answer += 2**exponential *(net(trn_X)==net.bias).float()*net.bias * previous_answer\n",
    "    previous_answer = (net(trn_X)==net.bias).float()\n",
    "acc = ( (answer>0.01).float().view(-1) == trn_Y.view(-1) ).sum().item()/n * 100\n",
    "print(f\"The accuracy is {acc :.3f} %\")\n",
    "##################################\n",
    "\n",
    "### combined polytopes\n",
    "plt.figure()\n",
    "rank = torch.zeros_like(rank)\n",
    "previous_rank = torch.ones_like(rank)\n",
    "for exponential, net in enumerate(network_list):\n",
    "    rank += 2**exponential * (net(grid).view(N,N).detach().cpu() == net.bias).float() * net.bias * previous_rank\n",
    "    previous_rank = (net(grid).view(N,N).detach().cpu() == net.bias).float()\n",
    "plt.contourf(x,y,(rank>0.01).float().cpu(), np.arange(-1,2,1), cmap='Greys') ###\n",
    "plt.colorbar()\n",
    "plt.contour(x,y,(rank>0.01).float().cpu(), cmap='Greens', linewidths=2) ###\n",
    "plt.scatter(trn_X[:int(n/2),0].cpu(),trn_X[:int(n/2),1].cpu(), marker='.', color=\"C0\", label=\"class 0\", s=25)\n",
    "plt.scatter(trn_X[int(n/2):,0].cpu(),trn_X[int(n/2):,1].cpu(), marker='.', color=\"C1\", label=\"class 1\", s=25)\n",
    "plt.title(\"The polytope-basis cover\")\n",
    "plt.legend()\n",
    "plt.savefig(folder_name_above+f\"/polytope_cover_{acc :.3f}%.png\")\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
